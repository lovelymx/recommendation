{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, TimestampType, FloatType, ArrayType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from features import LABEL_COLUMN, DISPLAY_ID_COLUMN, AD_ID_COLUMN, IS_LEAK_COLUMN, DISPLAY_ID_AND_IS_LEAK_ENCODED_COLUMN, CATEGORICAL_COLUMNS, DOC_CATEGORICAL_MULTIVALUED_COLUMNS, BOOL_COLUMNS, INT_COLUMNS, FLOAT_COLUMNS, FLOAT_COLUMNS_LOG_BIN_TRANSFORM, FLOAT_COLUMNS_SIMPLE_BIN_TRANSFORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/preprocessed/\"\n",
    "DATA_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/\"\n",
    "SPARK_TEMP_FOLDER = \"hdfs:/user/lzhao/data/outbrain/spark-temp/\"\n",
    "LOCAL_DATA_TFRECORDS_DIR = \"../data/outbrain/tfreccords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORFLOW_HADOOP=\"../tensorflow-hadoop-1.15.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fe2d2293e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf = conf.setMaster(\"local[*]\")\n",
    "# conf = conf.set(\"spark.app.name\", \"recommend-ctr\")\n",
    "conf = conf.set(\"spark.executor.memory\", \"40g\")\n",
    "conf = conf.set(\"spark.driver.memory\", \"150g\")\n",
    "# conf = conf.set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "# conf = conf.set(\"spark.executor.instances\", \"110\")\n",
    "conf.set(\"spark.jars\", TENSORFLOW_HADOOP)\n",
    "conf.set(\"spark.local.dir\", \"SPARK_TEMP_FOLDER\")\n",
    "#conf = conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_feature_names = ['event_weekend',\n",
    "                      'user_has_already_viewed_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_feature_names = ['user_views',\n",
    "                    'ad_views',\n",
    "                    'doc_views',\n",
    "                    'doc_event_days_since_published',\n",
    "                    'doc_event_hour',\n",
    "                    'doc_ad_days_since_published',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_feature_names = [\n",
    "                'pop_ad_id',\n",
    "                'pop_ad_id_conf',\n",
    "                'pop_ad_id_conf_multipl',\n",
    "                'pop_document_id',\n",
    "                'pop_document_id_conf',\n",
    "                'pop_document_id_conf_multipl',\n",
    "                'pop_publisher_id',\n",
    "                'pop_publisher_id_conf',\n",
    "                'pop_publisher_id_conf_multipl',\n",
    "                'pop_advertiser_id',\n",
    "                'pop_advertiser_id_conf',\n",
    "                'pop_advertiser_id_conf_multipl',\n",
    "                'pop_campain_id',\n",
    "                'pop_campain_id_conf',\n",
    "                'pop_campain_id_conf_multipl',\n",
    "                'pop_doc_event_doc_ad',\n",
    "                'pop_doc_event_doc_ad_conf',\n",
    "                'pop_doc_event_doc_ad_conf_multipl',\n",
    "                'pop_source_id',\n",
    "                'pop_source_id_conf',\n",
    "                'pop_source_id_conf_multipl',\n",
    "                'pop_source_id_country',\n",
    "                'pop_source_id_country_conf',\n",
    "                'pop_source_id_country_conf_multipl',\n",
    "                'pop_entity_id',\n",
    "                'pop_entity_id_conf',\n",
    "                'pop_entity_id_conf_multipl',\n",
    "                'pop_entity_id_country',\n",
    "                'pop_entity_id_country_conf',\n",
    "                'pop_entity_id_country_conf_multipl',\n",
    "                'pop_topic_id',\n",
    "                'pop_topic_id_conf',\n",
    "                'pop_topic_id_conf_multipl',\n",
    "                'pop_topic_id_country',\n",
    "                'pop_topic_id_country_conf',\n",
    "                'pop_topic_id_country_conf_multipl',\n",
    "                'pop_category_id',\n",
    "                'pop_category_id_conf',\n",
    "                'pop_category_id_conf_multipl',\n",
    "                'pop_category_id_country',\n",
    "                'pop_category_id_country_conf',\n",
    "                'pop_category_id_country_conf_multipl',\n",
    "                'user_doc_ad_sim_categories',\n",
    "                'user_doc_ad_sim_categories_conf',\n",
    "                'user_doc_ad_sim_categories_conf_multipl',\n",
    "                'user_doc_ad_sim_topics',\n",
    "                'user_doc_ad_sim_topics_conf',\n",
    "                'user_doc_ad_sim_topics_conf_multipl',\n",
    "                'user_doc_ad_sim_entities',\n",
    "                'user_doc_ad_sim_entities_conf',\n",
    "                'user_doc_ad_sim_entities_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_categories',\n",
    "                'doc_event_doc_ad_sim_categories_conf',\n",
    "                'doc_event_doc_ad_sim_categories_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_topics',\n",
    "                'doc_event_doc_ad_sim_topics_conf',\n",
    "                'doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_entities',\n",
    "                'doc_event_doc_ad_sim_entities_conf',\n",
    "                'doc_event_doc_ad_sim_entities_conf_multipl'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_feature_names_integral = ['ad_advertiser',\n",
    " 'doc_ad_category_id_1',\n",
    " 'doc_ad_category_id_2',\n",
    " 'doc_ad_category_id_3',\n",
    " 'doc_ad_topic_id_1',\n",
    " 'doc_ad_topic_id_2',\n",
    " 'doc_ad_topic_id_3',\n",
    " 'doc_ad_entity_id_1',\n",
    " 'doc_ad_entity_id_2',\n",
    " 'doc_ad_entity_id_3',\n",
    " 'doc_ad_entity_id_4',\n",
    " 'doc_ad_entity_id_5',\n",
    " 'doc_ad_entity_id_6',\n",
    " 'doc_ad_publisher_id',\n",
    " 'doc_ad_source_id',\n",
    " 'doc_event_category_id_1',\n",
    " 'doc_event_category_id_2',\n",
    " 'doc_event_category_id_3',\n",
    " 'doc_event_topic_id_1',\n",
    " 'doc_event_topic_id_2',\n",
    " 'doc_event_topic_id_3',\n",
    " 'doc_event_entity_id_1',\n",
    " 'doc_event_entity_id_2',\n",
    " 'doc_event_entity_id_3',\n",
    " 'doc_event_entity_id_4',\n",
    " 'doc_event_entity_id_5',\n",
    " 'doc_event_entity_id_6',\n",
    " 'doc_event_publisher_id',\n",
    " 'doc_event_source_id',\n",
    " 'event_country',\n",
    " 'event_country_state',\n",
    " 'event_geo_location',\n",
    " 'event_hour',\n",
    " 'event_platform',\n",
    " 'traffic_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_labels_integral = bool_feature_names + int_feature_names + float_feature_names + category_feature_names_integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "\n",
    "submission = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if submission:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral'\n",
    "else:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid='10005a0add15f6', display_id=5686397, ad_id=174547, document_id_event=2135921, document_id=1439845, label=0, feature_vector=SparseVector(103, {0: 1.0, 3: 87111.0, 4: 106124.0, 5: 1.0, 6: 4.0, 7: 124.0, 8: 0.0913, 9: 0.6848, 10: 0.0625, 11: 0.0831, 12: 0.4983, 13: 0.0414, 14: 0.0534, 15: 0.4446, 16: 0.0237, 17: 0.0455, 18: 0.42, 19: 0.0191, 20: 0.0865, 21: 0.5491, 22: 0.0475, 26: 0.0534, 27: 0.4446, 28: 0.0237, 29: 0.0563, 30: 0.4569, 31: 0.0257, 32: 0.0935, 33: 0.0759, 34: 0.0071, 35: 0.0966, 36: 0.0778, 37: 0.0075, 38: 0.1646, 39: 0.0002, 40: 0.0, 41: 0.1629, 42: 0.0002, 43: 0.0, 44: 0.2065, 45: 0.1339, 46: 0.0277, 47: 0.212, 48: 0.1326, 49: 0.0281, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 68: 2151.0, 69: 1302.0, 70: 1207.0, 72: 2.0, 75: 552.0, 76: 372.0, 81: 2618.0, 82: 9698.0, 83: 1702.0, 84: 1707.0, 86: 137.0, 95: 723.0, 96: 4194.0, 97: 18595447.0, 98: 745661.0, 99: 33260.0, 100: 4.0, 101: 1.0, 102: 0.0})),\n",
       " Row(uuid='10005a0add15f6', display_id=5686397, ad_id=61941, document_id_event=2135921, document_id=1017869, label=1, feature_vector=SparseVector(103, {0: 1.0, 3: 11370.0, 4: 15083.0, 5: 1.0, 6: 4.0, 7: 173.0, 8: 0.4069, 9: 0.5623, 10: 0.2288, 11: 0.3902, 12: 0.5131, 13: 0.2002, 14: 0.2691, 15: 0.3531, 16: 0.095, 17: 0.3902, 18: 0.5131, 19: 0.2002, 20: 0.3902, 21: 0.5131, 22: 0.2002, 26: 0.3902, 27: 0.5131, 28: 0.2002, 29: 0.3902, 30: 0.5131, 31: 0.2002, 32: 0.3826, 33: 0.2833, 34: 0.1084, 35: 0.3962, 36: 0.2833, 37: 0.1122, 38: 0.2738, 39: 0.0012, 40: 0.0003, 41: 0.2801, 42: 0.0012, 43: 0.0003, 44: 0.2298, 45: 0.0622, 46: 0.0143, 47: 0.2336, 48: 0.0595, 49: 0.0139, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 68: 2413.0, 69: 1403.0, 70: 1610.0, 72: 108.0, 75: 194.0, 76: 15.0, 81: 440.0, 82: 4016.0, 83: 1702.0, 84: 1707.0, 86: 137.0, 95: 723.0, 96: 4194.0, 97: 18595447.0, 98: 745661.0, 99: 33260.0, 100: 4.0, 101: 1.0, 102: 0.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_vectors_exported_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_ORDERED_COLUMNS = ['label','display_id','ad_id','doc_id','doc_event_id','is_leak','event_weekend',\n",
    "              'user_has_already_viewed_doc','user_views','ad_views','doc_views',\n",
    "              'doc_event_days_since_published','doc_event_hour','doc_ad_days_since_published',\n",
    "              'pop_ad_id','pop_ad_id_conf',\n",
    "              'pop_ad_id_conf_multipl','pop_document_id','pop_document_id_conf',\n",
    "              'pop_document_id_conf_multipl','pop_publisher_id','pop_publisher_id_conf',\n",
    "              'pop_publisher_id_conf_multipl','pop_advertiser_id','pop_advertiser_id_conf',\n",
    "              'pop_advertiser_id_conf_multipl','pop_campain_id','pop_campain_id_conf',\n",
    "              'pop_campain_id_conf_multipl','pop_doc_event_doc_ad','pop_doc_event_doc_ad_conf',\n",
    "              'pop_doc_event_doc_ad_conf_multipl','pop_source_id','pop_source_id_conf',\n",
    "              'pop_source_id_conf_multipl','pop_source_id_country','pop_source_id_country_conf',\n",
    "              'pop_source_id_country_conf_multipl','pop_entity_id','pop_entity_id_conf',\n",
    "              'pop_entity_id_conf_multipl','pop_entity_id_country','pop_entity_id_country_conf',\n",
    "              'pop_entity_id_country_conf_multipl','pop_topic_id','pop_topic_id_conf',\n",
    "              'pop_topic_id_conf_multipl','pop_topic_id_country','pop_topic_id_country_conf',\n",
    "              'pop_topic_id_country_conf_multipl','pop_category_id','pop_category_id_conf',\n",
    "              'pop_category_id_conf_multipl','pop_category_id_country','pop_category_id_country_conf',\n",
    "              'pop_category_id_country_conf_multipl','user_doc_ad_sim_categories',\n",
    "              'user_doc_ad_sim_categories_conf','user_doc_ad_sim_categories_conf_multipl',\n",
    "              'user_doc_ad_sim_topics','user_doc_ad_sim_topics_conf','user_doc_ad_sim_topics_conf_multipl',\n",
    "              'user_doc_ad_sim_entities','user_doc_ad_sim_entities_conf','user_doc_ad_sim_entities_conf_multipl',\n",
    "              'doc_event_doc_ad_sim_categories','doc_event_doc_ad_sim_categories_conf',\n",
    "              'doc_event_doc_ad_sim_categories_conf_multipl','doc_event_doc_ad_sim_topics',\n",
    "              'doc_event_doc_ad_sim_topics_conf','doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "              'doc_event_doc_ad_sim_entities','doc_event_doc_ad_sim_entities_conf',\n",
    "              'doc_event_doc_ad_sim_entities_conf_multipl','ad_advertiser','doc_ad_category_id_1',\n",
    "              'doc_ad_category_id_2','doc_ad_category_id_3','doc_ad_topic_id_1','doc_ad_topic_id_2',\n",
    "              'doc_ad_topic_id_3','doc_ad_entity_id_1','doc_ad_entity_id_2','doc_ad_entity_id_3',\n",
    "              'doc_ad_entity_id_4','doc_ad_entity_id_5','doc_ad_entity_id_6','doc_ad_publisher_id',\n",
    "              'doc_ad_source_id','doc_event_category_id_1','doc_event_category_id_2','doc_event_category_id_3',\n",
    "              'doc_event_topic_id_1','doc_event_topic_id_2','doc_event_topic_id_3','doc_event_entity_id_1',\n",
    "              'doc_event_entity_id_2','doc_event_entity_id_3','doc_event_entity_id_4','doc_event_entity_id_5',\n",
    "              'doc_event_entity_id_6','doc_event_publisher_id','doc_event_source_id','event_country',\n",
    "              'event_country_state','event_geo_location','event_hour','event_platform','traffic_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_CSV_ORDERED_COLUMNS = ['event_weekend',\n",
    "              'user_has_already_viewed_doc','user_views','ad_views','doc_views',\n",
    "              'doc_event_days_since_published','doc_event_hour','doc_ad_days_since_published',\n",
    "              'pop_ad_id','pop_ad_id_conf',\n",
    "              'pop_ad_id_conf_multipl','pop_document_id','pop_document_id_conf',\n",
    "              'pop_document_id_conf_multipl','pop_publisher_id','pop_publisher_id_conf',\n",
    "              'pop_publisher_id_conf_multipl','pop_advertiser_id','pop_advertiser_id_conf',\n",
    "              'pop_advertiser_id_conf_multipl','pop_campain_id','pop_campain_id_conf',\n",
    "              'pop_campain_id_conf_multipl','pop_doc_event_doc_ad','pop_doc_event_doc_ad_conf',\n",
    "              'pop_doc_event_doc_ad_conf_multipl','pop_source_id','pop_source_id_conf',\n",
    "              'pop_source_id_conf_multipl','pop_source_id_country','pop_source_id_country_conf',\n",
    "              'pop_source_id_country_conf_multipl','pop_entity_id','pop_entity_id_conf',\n",
    "              'pop_entity_id_conf_multipl','pop_entity_id_country','pop_entity_id_country_conf',\n",
    "              'pop_entity_id_country_conf_multipl','pop_topic_id','pop_topic_id_conf',\n",
    "              'pop_topic_id_conf_multipl','pop_topic_id_country','pop_topic_id_country_conf',\n",
    "              'pop_topic_id_country_conf_multipl','pop_category_id','pop_category_id_conf',\n",
    "              'pop_category_id_conf_multipl','pop_category_id_country','pop_category_id_country_conf',\n",
    "              'pop_category_id_country_conf_multipl','user_doc_ad_sim_categories',\n",
    "              'user_doc_ad_sim_categories_conf','user_doc_ad_sim_categories_conf_multipl',\n",
    "              'user_doc_ad_sim_topics','user_doc_ad_sim_topics_conf','user_doc_ad_sim_topics_conf_multipl',\n",
    "              'user_doc_ad_sim_entities','user_doc_ad_sim_entities_conf','user_doc_ad_sim_entities_conf_multipl',\n",
    "              'doc_event_doc_ad_sim_categories','doc_event_doc_ad_sim_categories_conf',\n",
    "              'doc_event_doc_ad_sim_categories_conf_multipl','doc_event_doc_ad_sim_topics',\n",
    "              'doc_event_doc_ad_sim_topics_conf','doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "              'doc_event_doc_ad_sim_entities','doc_event_doc_ad_sim_entities_conf',\n",
    "              'doc_event_doc_ad_sim_entities_conf_multipl','ad_advertiser','doc_ad_category_id_1',\n",
    "              'doc_ad_category_id_2','doc_ad_category_id_3','doc_ad_topic_id_1','doc_ad_topic_id_2',\n",
    "              'doc_ad_topic_id_3','doc_ad_entity_id_1','doc_ad_entity_id_2','doc_ad_entity_id_3',\n",
    "              'doc_ad_entity_id_4','doc_ad_entity_id_5','doc_ad_entity_id_6','doc_ad_publisher_id',\n",
    "              'doc_ad_source_id','doc_event_category_id_1','doc_event_category_id_2','doc_event_category_id_3',\n",
    "              'doc_event_topic_id_1','doc_event_topic_id_2','doc_event_topic_id_3','doc_event_entity_id_1',\n",
    "              'doc_event_entity_id_2','doc_event_entity_id_3','doc_event_entity_id_4','doc_event_entity_id_5',\n",
    "              'doc_event_entity_id_6','doc_event_publisher_id','doc_event_source_id','event_country',\n",
    "              'event_country_state','event_geo_location','event_hour','event_platform','traffic_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    " \n",
    "    return F.udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT_TO_INT = ['doc_ad_category_id_1',\n",
    "              'doc_ad_category_id_2','doc_ad_category_id_3','doc_ad_topic_id_1','doc_ad_topic_id_2',\n",
    "              'doc_ad_topic_id_3','doc_ad_entity_id_1','doc_ad_entity_id_2','doc_ad_entity_id_3',\n",
    "              'doc_ad_entity_id_4','doc_ad_entity_id_5','doc_ad_entity_id_6',\n",
    "              'doc_ad_source_id','doc_event_category_id_1','doc_event_category_id_2','doc_event_category_id_3',\n",
    "              'doc_event_topic_id_1','doc_event_topic_id_2','doc_event_topic_id_3','doc_event_entity_id_1',\n",
    "              'doc_event_entity_id_2','doc_event_entity_id_3','doc_event_entity_id_4','doc_event_entity_id_5', 'doc_event_entity_id_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(element, name):\n",
    "    if name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        return element.cast(\"int\")\n",
    "    elif name in CONVERT_TO_INT:\n",
    "        return element.cast(\"int\")\n",
    "    else:\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array_with_none(col):\n",
    "    def to_array_with_none_(v):\n",
    "        tmp= np.full((v.size,), fill_value=None, dtype=np.float64)\n",
    "        tmp[v.indices] = v.values\n",
    "        return tmp.tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    " \n",
    "    return F.udf(to_array_with_none_, ArrayType(DoubleType())).asNondeterministic()(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_most_frequent(most_value):\n",
    "    return F.udf( lambda x: most_value if not x or np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_vectors_integral_csv_rdd_df = train_feature_vectors_exported_df \\\n",
    ".select('label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'feature_vector') \\\n",
    ".withColumn('is_leak', F.lit(-1)).withColumn(\"featvec\", to_array(\"feature_vector\")) \\\n",
    ".select(['label'] + ['display_id'] + ['ad_id'] + ['document_id'] + ['document_id_event'] + ['is_leak'] + [format_number(element, FEAT_CSV_ORDERED_COLUMNS[index]).alias(FEAT_CSV_ORDERED_COLUMNS[index]) for index, element in enumerate([F.col(\"featvec\")[i] for i in range(len(feature_vector_labels_integral))])])\\\n",
    ".replace(float('nan'), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, display_id=5686397, ad_id=174547, document_id=1439845, document_id_event=2135921, is_leak=-1, event_weekend=1, user_has_already_viewed_doc=0, user_views=0.0, ad_views=87111.0, doc_views=106124.0, doc_event_days_since_published=1.0, doc_event_hour=4.0, doc_ad_days_since_published=124.0, pop_ad_id=0.09133175015449524, pop_ad_id_conf=0.6848396305407448, pop_ad_id_conf_multipl=0.06254760203244414, pop_document_id=0.08310090005397797, pop_document_id_conf=0.4983114381899986, pop_document_id_conf_multipl=0.041410129020781096, pop_publisher_id=0.05335606634616852, pop_publisher_id_conf=0.4445772362756268, pop_publisher_id_conf_multipl=0.02372089251471858, pop_advertiser_id=0.04552464559674263, pop_advertiser_id_conf=0.4199930658825777, pop_advertiser_id_conf_multipl=0.019120035477393726, pop_campain_id=0.08649083971977234, pop_campain_id_conf=0.549118229426116, pop_campain_id_conf_multipl=0.04749369676849937, pop_doc_event_doc_ad=0.0, pop_doc_event_doc_ad_conf=0.0, pop_doc_event_doc_ad_conf_multipl=0.0, pop_source_id=0.05335606634616852, pop_source_id_conf=0.4445772362756268, pop_source_id_conf_multipl=0.02372089251471858, pop_source_id_country=0.05628378689289093, pop_source_id_country_conf=0.45693757257696255, pop_source_id_country_conf_multipl=0.025718176958276644, pop_entity_id=0.09346514576630129, pop_entity_id_conf=0.07591298866466077, pop_entity_id_conf_multipl=0.007095218551098096, pop_entity_id_country=0.09656113447077769, pop_entity_id_country_conf=0.07779373328496297, pop_entity_id_country_conf_multipl=0.007511851140713123, pop_topic_id=0.16457937657833102, pop_topic_id_conf=0.00021974087322175189, pop_topic_id_conf_multipl=3.6164815923614e-05, pop_topic_id_country=0.16291247308254245, pop_topic_id_country_conf=0.00023780887004633665, pop_topic_id_country_conf_multipl=3.8742031140213654e-05, pop_category_id=0.20645902142705938, pop_category_id_conf=0.13393374679662998, pop_category_id_conf_multipl=0.027651830299691774, pop_category_id_country=0.21203752748219867, pop_category_id_country_conf=0.13263788652057232, pop_category_id_country_conf_multipl=0.0281242095082866, user_doc_ad_sim_categories=0.0, user_doc_ad_sim_categories_conf=0.0, user_doc_ad_sim_categories_conf_multipl=0.0, user_doc_ad_sim_topics=0.0, user_doc_ad_sim_topics_conf=0.0, user_doc_ad_sim_topics_conf_multipl=0.0, user_doc_ad_sim_entities=0.0, user_doc_ad_sim_entities_conf=0.0, user_doc_ad_sim_entities_conf_multipl=0.0, doc_event_doc_ad_sim_categories=0.0, doc_event_doc_ad_sim_categories_conf=0.00042512488043366936, doc_event_doc_ad_sim_categories_conf_multipl=0.0, doc_event_doc_ad_sim_topics=0.0, doc_event_doc_ad_sim_topics_conf=1.111111111107288e-05, doc_event_doc_ad_sim_topics_conf_multipl=0.0, doc_event_doc_ad_sim_entities=0.0, doc_event_doc_ad_sim_entities_conf=0.0, doc_event_doc_ad_sim_entities_conf_multipl=0.0, ad_advertiser=2151, doc_ad_category_id_1=1302, doc_ad_category_id_2=1207, doc_ad_category_id_3=0, doc_ad_topic_id_1=2, doc_ad_topic_id_2=0, doc_ad_topic_id_3=0, doc_ad_entity_id_1=552, doc_ad_entity_id_2=372, doc_ad_entity_id_3=0, doc_ad_entity_id_4=0, doc_ad_entity_id_5=0, doc_ad_entity_id_6=0, doc_ad_publisher_id=2618, doc_ad_source_id=9698, doc_event_category_id_1=1702, doc_event_category_id_2=1707, doc_event_category_id_3=0, doc_event_topic_id_1=137, doc_event_topic_id_2=0, doc_event_topic_id_3=0, doc_event_entity_id_1=0, doc_event_entity_id_2=0, doc_event_entity_id_3=0, doc_event_entity_id_4=0, doc_event_entity_id_5=0, doc_event_entity_id_6=0, doc_event_publisher_id=723, doc_event_source_id=4194, event_country=18595447, event_country_state=745661, event_geo_location=33260, event_hour=4, event_platform=1, traffic_source=0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_vectors_integral_csv_rdd_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if submission:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'test_feature_vectors_integral'\n",
    "else:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'validation_feature_vectors_integral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid='100289071872c9', display_id=3110701, ad_id=327512, document_id_event=1973614, document_id=1286118, label=0, is_leak=0, feature_vector=SparseVector(103, {0: 0.0, 1: 0.0, 2: 43.0, 3: 1316.0, 4: 45696.0, 5: 1.0, 6: 4.0, 7: 79.0, 8: 0.2371, 9: 0.4325, 10: 0.1025, 11: 0.2615, 12: 0.3102, 13: 0.0811, 14: 0.2415, 15: 0.3236, 16: 0.0782, 17: 0.2415, 18: 0.3236, 19: 0.0782, 20: 0.2145, 21: 0.3359, 22: 0.072, 23: 0.0, 24: 0.0575, 25: 0.0, 26: 0.2415, 27: 0.3236, 28: 0.0782, 29: 0.2439, 30: 0.3463, 31: 0.0845, 32: 0.2555, 33: 0.0076, 34: 0.002, 35: 0.2641, 36: 0.0079, 37: 0.0021, 38: 0.2629, 39: 0.014, 40: 0.0037, 41: 0.2311, 42: 0.0166, 43: 0.0038, 44: 0.2299, 45: 0.0741, 46: 0.017, 47: 0.2324, 48: 0.0709, 49: 0.0165, 50: 0.9143, 51: 1.0, 52: 0.9143, 53: 0.0, 54: 0.0005, 55: 0.0, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0.9659, 60: 0.9996, 61: 0.9655, 62: 0.0, 63: 0.0, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 2635.0, 69: 1403.0, 70: 1406.0, 72: 227.0, 75: 429.0, 81: 5911.0, 82: 8905.0, 83: 1403.0, 84: 1408.0, 86: 136.0, 95: 407.0, 96: 6715.0, 97: 18595447.0, 98: 1645323.0, 99: 428139.0, 100: 4.0, 101: 1.0, 102: 0.0})),\n",
       " Row(uuid='100289071872c9', display_id=3110701, ad_id=155861, document_id_event=1973614, document_id=1387388, label=0, is_leak=0, feature_vector=SparseVector(103, {0: 0.0, 1: 0.0, 2: 43.0, 3: 514.0, 4: 516.0, 5: 1.0, 6: 4.0, 7: 43.0, 8: 0.3132, 9: 0.3759, 10: 0.1178, 11: 0.312, 12: 0.3346, 13: 0.1044, 14: 0.2365, 15: 0.2692, 16: 0.0637, 17: 0.3281, 18: 0.2958, 19: 0.097, 20: 0.3448, 21: 0.3375, 22: 0.1164, 26: 0.2469, 27: 0.2635, 28: 0.0651, 29: 0.2687, 30: 0.2742, 31: 0.0737, 38: 0.1993, 39: 0.0019, 40: 0.0004, 41: 0.2036, 42: 0.0021, 43: 0.0004, 44: 0.2285, 45: 0.0828, 46: 0.0189, 47: 0.2323, 48: 0.0791, 49: 0.0184, 50: 0.9237, 51: 0.996, 52: 0.92, 53: 0.0, 54: 0.001, 55: 0.0, 59: 0.9785, 60: 0.9996, 61: 0.9781, 62: 0.0, 63: 0.0, 64: 0.0, 68: 232.0, 69: 1403.0, 70: 1513.0, 72: 173.0, 73: 192.0, 81: 503.0, 82: 4686.0, 83: 1403.0, 84: 1408.0, 86: 136.0, 95: 407.0, 96: 6715.0, 97: 18595447.0, 98: 1645323.0, 99: 428139.0, 100: 4.0, 101: 1.0, 102: 0.0}))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_validation_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name)\n",
    "test_validation_feature_vectors_exported_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_feature_vectors_integral_csv_rdd_df = test_validation_feature_vectors_exported_df.select(\n",
    "    'label', 'display_id', 'ad_id', 'document_id', 'document_id_event',\n",
    "    'is_leak', 'feature_vector').withColumn(\"featvec\", to_array(\"feature_vector\")).select(['label'] + ['display_id'] + ['ad_id'] + ['document_id'] + ['document_id_event'] + ['is_leak'] + [format_number(element, FEAT_CSV_ORDERED_COLUMNS[index]).alias(FEAT_CSV_ORDERED_COLUMNS[index]) for index, element in enumerate([F.col(\"featvec\")[i] for i in range(len(feature_vector_labels_integral))])]).replace(float('nan'), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import metadata_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec(output_dir, batch_size=None):\n",
    "    fixed_shape = [batch_size,1] if batch_size is not None else []\n",
    "    spec = {}\n",
    "    spec[LABEL_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    spec[DISPLAY_ID_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    spec[IS_LEAK_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    spec[DISPLAY_ID_AND_IS_LEAK_ENCODED_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for name in BOOL_COLUMNS:\n",
    "        spec[name] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM+FLOAT_COLUMNS_SIMPLE_BIN_TRANSFORM:\n",
    "        spec[name] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.float32, default_value=None)\n",
    "    for name in FLOAT_COLUMNS_SIMPLE_BIN_TRANSFORM:\n",
    "        spec[name + '_binned'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM:\n",
    "        spec[name + '_binned'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "        spec[name + '_log_01scaled'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.float32, default_value=None)\n",
    "    for name in INT_COLUMNS:\n",
    "        spec[name + '_log_int'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "        spec[name + '_log_01scaled'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.float32, default_value=None)\n",
    "    for name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        spec[name] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for multi_category in DOC_CATEGORICAL_MULTIVALUED_COLUMNS:\n",
    "        shape = fixed_shape[:-1]+[len(DOC_CATEGORICAL_MULTIVALUED_COLUMNS[multi_category])]\n",
    "        spec[multi_category] = tf.io.FixedLenFeature(shape=shape, dtype=tf.int64)\n",
    "    metadata = dataset_metadata.DatasetMetadata(dataset_schema.from_feature_spec(spec))\n",
    "    metadata_io.write_metadata(metadata, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-9820ecd26ade>:25: from_feature_spec (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "from_feature_spec is a deprecated, use schema_utils.schema_from_feature_spec\n"
     ]
    }
   ],
   "source": [
    "make_spec(LOCAL_DATA_TFRECORDS_DIR + '/transformed_metadata', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2_1p(x):\n",
    "    return np.log1p(x) / np.log(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_max_logs(df):\n",
    "    print(str(datetime.datetime.now()) + '\\tComputing min and max')\n",
    "    min_logs = {}\n",
    "    max_logs = {}\n",
    "    all_dict = {}\n",
    "    float_expr = []\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM + INT_COLUMNS:\n",
    "        float_expr.append(F.min(name))\n",
    "        float_expr.append(F.max(name))\n",
    "    floatDf = df.agg(*float_expr).collect()\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM:\n",
    "        minAgg = floatDf[0][\"min(\"+name+\")\"]\n",
    "        maxAgg = floatDf[0][\"max(\"+name+\")\"]\n",
    "        min_logs[name + '_log_01scaled'] = log2_1p(minAgg*1000)\n",
    "        max_logs[name + '_log_01scaled'] = log2_1p(maxAgg*1000)\n",
    "    for name in  INT_COLUMNS:\n",
    "        minAgg = floatDf[0][\"min(\"+name+\")\"]\n",
    "        maxAgg = floatDf[0][\"max(\"+name+\")\"]\n",
    "        min_logs[name + '_log_01scaled'] = log2_1p(minAgg)\n",
    "        max_logs[name + '_log_01scaled'] = log2_1p(maxAgg)\n",
    "\n",
    "    return min_logs, max_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = test_validation_feature_vectors_integral_csv_rdd_df.union(train_feature_vectors_integral_csv_rdd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 12:33:07.742072\tComputing min and max\n"
     ]
    }
   ],
   "source": [
    "min_logs, max_logs = compute_min_max_logs(all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if submission:\n",
    "    train_output_string = '/sub_train'\n",
    "    eval_output_string = '/test'\n",
    "else:\n",
    "    train_output_string = '/train'\n",
    "    eval_output_string = '/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = LOCAL_DATA_TFRECORDS_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example_spark(df, min_logs, max_logs):\n",
    "    result = {}\n",
    "    result[LABEL_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=df[LABEL_COLUMN].to_list()))\n",
    "    result[DISPLAY_ID_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=df[DISPLAY_ID_COLUMN].to_list()))\n",
    "    result[IS_LEAK_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=df[IS_LEAK_COLUMN].to_list()))\n",
    "    encoded_value = df[DISPLAY_ID_COLUMN].multiply(10).add(df[IS_LEAK_COLUMN].clip(lower=0)).to_list()\n",
    "    result[DISPLAY_ID_AND_IS_LEAK_ENCODED_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=encoded_value))\n",
    "    for name in FLOAT_COLUMNS:\n",
    "        value = df[name].to_list()\n",
    "        result[name] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    for name in FLOAT_COLUMNS_SIMPLE_BIN_TRANSFORM:\n",
    "        value = df[name].multiply(10).astype('int64').to_list()\n",
    "        result[name + '_binned'] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM:\n",
    "        value_prelim = df[name].multiply(1000).apply(np.log1p).multiply(1./np.log(2.0))\n",
    "        value = value_prelim.astype('int64').to_list()\n",
    "        result[name + '_binned'] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "        nn = name + '_log_01scaled'\n",
    "        value = value_prelim.add(-min_logs[nn]).multiply(1./(max_logs[nn]-min_logs[nn])).to_list()\n",
    "        result[nn] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    for name in INT_COLUMNS:\n",
    "        value_prelim = df[name].apply(np.log1p).multiply(1./np.log(2.0))\n",
    "        value = value_prelim.astype('int64').to_list()\n",
    "        result[name + '_log_int'] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "        nn = name + '_log_01scaled'\n",
    "        value = value_prelim.add(-min_logs[nn]).multiply(1./(max_logs[nn]-min_logs[nn])).to_list()\n",
    "        result[nn] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    for name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        value = df[name].fillna(0).astype('int64').to_list()\n",
    "        result[name] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    for multi_category in DOC_CATEGORICAL_MULTIVALUED_COLUMNS:\n",
    "        values = []\n",
    "        for category in DOC_CATEGORICAL_MULTIVALUED_COLUMNS[multi_category]:\n",
    "            values = values + [df[category].to_numpy()]\n",
    "    # need to transpose the series so they will be parsed correctly by the FixedLenFeature\n",
    "    # we can pass in a single series here; they'll be reshaped to [batch_size, num_values]\n",
    "    # when parsed from the TFRecord\n",
    "        value = np.stack(values, axis=1).flatten().tolist()\n",
    "        result[multi_category] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=result))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_to_tfrecords(rdds):\n",
    "    csv = pd.DataFrame(list(rdds), columns=CSV_ORDERED_COLUMNS)\n",
    "    num_rows = len(csv.index)\n",
    "    examples = []\n",
    "    for start_ind in range(0,num_rows,batch_size if batch_size is not None else 1): # for each batch\n",
    "        if start_ind + batch_size - 1 > num_rows: # if we'd run out of rows\n",
    "            csv_slice = csv.iloc[start_ind:] \n",
    "            # drop the remainder\n",
    "            print(\"last Example has: \", len(csv_slice))\n",
    "            examples.append((create_tf_example_spark(csv_slice, min_logs, max_logs), len(csv_slice)))\n",
    "            return examples\n",
    "        else:\n",
    "            csv_slice = csv.iloc[start_ind:start_ind+(batch_size if batch_size is not None else 1)]\n",
    "        examples.append((create_tf_example_spark(csv_slice, min_logs, max_logs), batch_size))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import TaskContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_partition_num = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_to_slices(rdds):\n",
    "    taskcontext = TaskContext.get()\n",
    "    partitionid = taskcontext.partitionId()\n",
    "    csv = pd.DataFrame(list(rdds), columns=CSV_ORDERED_COLUMNS)\n",
    "    num_rows = len(csv.index)\n",
    "    print(\"working with partition: \", partitionid, max_partition_num, num_rows)\n",
    "    examples = []\n",
    "    for start_ind in range(0,num_rows,batch_size if batch_size is not None else 1): # for each batch\n",
    "        if start_ind + batch_size - 1 > num_rows: # if we'd run out of rows\n",
    "            csv_slice = csv.iloc[start_ind:] \n",
    "            print(\"last Example has: \", len(csv_slice), partitionid)\n",
    "            examples.append((csv_slice, len(csv_slice)))\n",
    "            return examples\n",
    "        else:\n",
    "            csv_slice = csv.iloc[start_ind:start_ind+(batch_size if batch_size is not None else 1)]\n",
    "        examples.append((csv_slice, len(csv_slice)))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_to_tfrecords_from_slices(rdds):\n",
    "    examples = []\n",
    "    for slice in rdds:\n",
    "        if len(slice[0]) != batch_size:\n",
    "            print(\"slice size is not correct, dropping: \", len(slice[0]))\n",
    "        else:\n",
    "            examples.append((bytearray((create_tf_example_spark(slice[0], min_logs, max_logs)).SerializeToString()), None))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_to_tfrecords_from_reslice(rdds):\n",
    "    examples = []\n",
    "    all_dataframes = pd.DataFrame([])\n",
    "    for slice in rdds:\n",
    "        all_dataframes = all_dataframes.append(slice[0])\n",
    "        num_rows = len(all_dataframes.index)\n",
    "    examples = []\n",
    "    for start_ind in range(0,num_rows,batch_size if batch_size is not None else 1): # for each batch\n",
    "        if start_ind + batch_size - 1 > num_rows: # if we'd run out of rows\n",
    "            csv_slice = all_dataframes.iloc[start_ind:]\n",
    "            if TEST_SET_MODE:\n",
    "                remain_len = batch_size - len(csv_slice)\n",
    "                (m, n) = divmod(remain_len, len(csv_slice))\n",
    "                print(\"remainder: \", len(csv_slice), remain_len, m, n)\n",
    "                if m:\n",
    "                    for i in range(m):\n",
    "                        csv_slice = csv_slice.append(csv_slice)\n",
    "                csv_slice = csv_slice.append(csv_slice.iloc[:n])\n",
    "                print(\"after fill remainder: \", len(csv_slice))\n",
    "                examples.append((bytearray((create_tf_example_spark(csv_slice, min_logs, max_logs)).SerializeToString()), None))\n",
    "                return examples\n",
    "            # drop the remainder\n",
    "            print(\"dropping remainder: \", len(csv_slice))\n",
    "            return examples\n",
    "        else:\n",
    "            csv_slice = all_dataframes.iloc[start_ind:start_ind+(batch_size if batch_size is not None else 1)]\n",
    "        examples.append((bytearray((create_tf_example_spark(csv_slice, min_logs, max_logs)).SerializeToString()), None))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_MODE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_feature_vectors_integral_csv_rdd_df.coalesce(30).rdd.mapPartitions(_transform_to_slices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train_features = train_features.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
