{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, TimestampType, FloatType, ArrayType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = conf.setMaster(\"yarn\")\n",
    "conf = conf.set(\"spark.app.name\", \"recommend-ctr\")\n",
    "conf = conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf = conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf = conf.set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "conf = conf.set(\"spark.executor.instances\", \"100\")\n",
    "conf = conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/preprocessed/\"\n",
    "DATA_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/\"\n",
    "SPARK_TEMP_FOLDER = \"hdfs:/user/lzhao/data/outbrain/spark-temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_meta_df = spark.read.schema(documents_meta_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "  .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop rows with empty \"source_id\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Drop rows with empty \"source_id\"...')\n",
    "documents_meta_df = documents_meta_df.dropna(subset=\"source_id\")\n",
    "documents_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14394"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_publishers_df = documents_meta_df.select([\"source_id\", \"publisher_id\"]).dropDuplicates()\n",
    "source_publishers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get list of source_ids without publisher_id...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5058"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Get list of source_ids without publisher_id...')\n",
    "rows_no_pub = source_publishers_df.filter(\"publisher_id is NULL\")\n",
    "source_ids_without_publisher = [row['source_id'] for row in rows_no_pub.collect()]\n",
    "len(source_ids_without_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of publisher_id used so far...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Maximum value of publisher_id used so far...')\n",
    "max_pub = max(source_publishers_df.select([\"publisher_id\"]).dropna().collect())['publisher_id']\n",
    "max_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows filled with new publisher_ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(source_id=5803, publisher_id=1264),\n",
       " Row(source_id=7754, publisher_id=1265),\n",
       " Row(source_id=7833, publisher_id=1266),\n",
       " Row(source_id=8086, publisher_id=1267),\n",
       " Row(source_id=3918, publisher_id=1268),\n",
       " Row(source_id=1088, publisher_id=1269),\n",
       " Row(source_id=13285, publisher_id=1270),\n",
       " Row(source_id=13289, publisher_id=1271),\n",
       " Row(source_id=13623, publisher_id=1272),\n",
       " Row(source_id=13832, publisher_id=1273)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Rows filled with new publisher_ids')\n",
    "new_publishers = [(source, max_pub + 1 + nr) for nr, source in enumerate(source_ids_without_publisher)]\n",
    "new_publishers_df = spark.createDataFrame(new_publishers, (\"source_id\", \"publisher_id\"))\n",
    "new_publishers_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source_id=10867, publisher_id=6312),\n",
       " Row(source_id=11292, publisher_id=6313),\n",
       " Row(source_id=11817, publisher_id=6314),\n",
       " Row(source_id=12141, publisher_id=6315),\n",
       " Row(source_id=12160, publisher_id=6316),\n",
       " Row(source_id=12285, publisher_id=6317),\n",
       " Row(source_id=12444, publisher_id=6318),\n",
       " Row(source_id=12622, publisher_id=6319),\n",
       " Row(source_id=12916, publisher_id=6320),\n",
       " Row(source_id=12991, publisher_id=6321)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_source_publishers_df = source_publishers_df.dropna().union(new_publishers_df)\n",
    "fixed_source_publishers_df.collect()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update documents_meta with bew publishers...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Update documents_meta with bew publishers...')\n",
    "documents_meta_df = documents_meta_df.drop('publisher_id').join(fixed_source_publishers_df, on='source_id')\n",
    "documents_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+------------------+------------+\n",
      "|source_id|document_id_doc|       publish_time|dummyDocumentsMeta|publisher_id|\n",
      "+---------+---------------+-------------------+------------------+------------+\n",
      "|       26|        2287601|2015-10-15 20:00:00|                 1|        1720|\n",
      "|       29|        1756309|2016-06-13 00:00:00|                 1|        1161|\n",
      "+---------+---------------+-------------------+------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_meta_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_df = spark.read.schema(documents_categories_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "  .alias('documents_categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "  .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "    F.collect_list('confidence_level_cat').alias('cat_confidence_level_list')) \\\n",
    "  .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "  .alias('documents_categories_grouped')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(document_id_cat=148, category_id_list=[1403, 1702], cat_confidence_level_list=[0.9200000166893005, 0.07000000029802322], dummyDocumentsCategory=1),\n",
       " Row(document_id_cat=463, category_id_list=[1513, 1808], cat_confidence_level_list=[0.8932095170021057, 0.06796159595251083], dummyDocumentsCategory=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_categories_grouped_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics_df = spark.read.schema(documents_topics_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\") \\\n",
    "  .alias('documents_topics')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "  .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "    F.collect_list('confidence_level_top').alias('top_confidence_level_list')) \\\n",
    "  .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "  .alias('documents_topics_grouped') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(document_id_top=148, topic_id_list=[153, 140, 8, 172, 244, 179, 36, 2, 64, 10, 216], top_confidence_level_list=[0.0752369686961174, 0.0719832107424736, 0.06108427047729492, 0.042060330510139465, 0.03971264883875847, 0.03684856742620468, 0.030745454132556915, 0.026049140840768814, 0.01605464331805706, 0.010918059386312962, 0.008370612747967243], dummyDocumentsTopics=1),\n",
       " Row(document_id_top=463, topic_id_list=[181, 292, 24, 254, 167], top_confidence_level_list=[0.1187012791633606, 0.05149438977241516, 0.04749272018671036, 0.021316789090633392, 0.008210956119000912], dummyDocumentsTopics=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_topics_grouped_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_entities_df = spark.read.schema(documents_entities_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\") \\\n",
    "  .alias('documents_entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "  .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "    F.collect_list('confidence_level_ent').alias('ent_confidence_level_list')) \\\n",
    "  .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "  .alias('documents_entities_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df = documents_meta_df.join(\n",
    "    documents_categories_grouped_df, \n",
    "    on=F.col(\"document_id_doc\") == F.col(\"documents_categories_grouped.document_id_cat\"), \n",
    "    how='left') \\\n",
    "  .join(documents_topics_grouped_df, \n",
    "    on=F.col(\"document_id_doc\") == F.col(\"documents_topics_grouped.document_id_top\"), \n",
    "    how='left') \\\n",
    "  .join(documents_entities_grouped_df, \n",
    "    on=F.col(\"document_id_doc\") == F.col(\"documents_entities_grouped.document_id_ent\"), \n",
    "    how='left') \\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source_id=1787, document_id_doc=148, publish_time=datetime.datetime(2008, 6, 30, 16, 0), dummyDocumentsMeta=1, publisher_id=118, document_id_cat=148, category_id_list=[1403, 1702], cat_confidence_level_list=[0.9200000166893005, 0.07000000029802322], dummyDocumentsCategory=1, document_id_top=148, topic_id_list=[153, 140, 8, 172, 244, 179, 36, 2, 64, 10, 216], top_confidence_level_list=[0.0752369686961174, 0.0719832107424736, 0.06108427047729492, 0.042060330510139465, 0.03971264883875847, 0.03684856742620468, 0.030745454132556915, 0.026049140840768814, 0.01605464331805706, 0.010918059386312962, 0.008370612747967243], dummyDocumentsTopics=1, document_id_ent=148, entity_id_list=['e1c74838563ef5d205063b0d95afa414', '6fd68f102042c6554cb2592fae942264', 'ae3de5466bfa10459eebcbe02ac9b3ee', '9da9595caa381755c9353ae7179f2117', 'b973c2e55831fb4025003e0259aa820f', '6eb92e281e46d463ce80317efd785d68', 'c323569535ca4c3d2ce474f4d825cc80', 'daf2f4c9cd8dbf10482f06200e613939'], ent_confidence_level_list=[0.6320257782936096, 0.4049091041088104, 0.2763647139072418, 0.2726665735244751, 0.26643022894859314, 0.25995519757270813, 0.2214961051940918, 0.21287654340267181], dummyDocumentsEntities=1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    validation_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "    .alias('validation_set')        \n",
    "  \n",
    "    validation_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile') \n",
    "    validation_set_df.select('uuid_event','document_id_promo').distinct() \\\n",
    "    .createOrReplaceTempView('validation_users_docs_to_ignore')\n",
    "else:\n",
    "    events_schema = StructType(\n",
    "                  [StructField(\"display_id\", IntegerType(), True),\n",
    "                  StructField(\"uuid_event\", StringType(), True),                    \n",
    "                  StructField(\"document_id_event\", IntegerType(), True),\n",
    "                  StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                  StructField(\"platform_event\", IntegerType(), True),\n",
    "                  StructField(\"geo_location_event\", StringType(), True)]\n",
    "                  )\n",
    "\n",
    "    events_df = spark.read.schema(events_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER+\"events.csv\") \\\n",
    "    .withColumn('dummyEvents', F.lit(1)) \\\n",
    "    .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "    .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "    .alias('events')\n",
    "\n",
    "  # Drop rows with empty \"geo_location\"\n",
    "    events_df = events_df.dropna(subset=\"geo_location_event\")\n",
    "  # Drop rows with empty \"platform\"\n",
    "    events_df = events_df.dropna(subset=\"platform_event\")\n",
    "\n",
    "    events_df.createOrReplaceTempView('events')\n",
    "\n",
    "\n",
    "    promoted_content_schema = StructType(\n",
    "                      [StructField(\"ad_id\", IntegerType(), True),\n",
    "                      StructField(\"document_id_promo\", IntegerType(), True),\n",
    "                      StructField(\"campaign_id\", IntegerType(), True),\n",
    "                      StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                      )\n",
    "\n",
    "    promoted_content_df = spark.read.schema(promoted_content_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "    .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content')\n",
    "\n",
    "    clicks_test_schema = StructType(\n",
    "                      [StructField(\"display_id\", IntegerType(), True),\n",
    "                      StructField(\"ad_id\", IntegerType(), True)]\n",
    "                      )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER+\"clicks_test.csv\") \\\n",
    "    .withColumn('dummyClicksTest', F.lit(1)).alias('clicks_test')\n",
    "  \n",
    "    test_set_df = clicks_test_df.join(promoted_content_df, on='ad_id', how='left') \\\n",
    "    .join(events_df, on='display_id', how='left')\n",
    "    test_set_df.cache()\n",
    "      \n",
    "    test_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')\n",
    "    test_set_df.select('uuid_event','document_id_promo', 'timestamp_event').distinct() \\\n",
    "    .createOrReplaceTempView('test_users_docs_timestamp_to_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+-----------------+-----------+-------------+--------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+\n",
      "|display_id| ad_id|dummyClicksTest|document_id_promo|campaign_id|advertiser_id|dummyPromotedContent|    uuid_event|document_id_event|timestamp_event|platform_event|geo_location_event|dummyEvents|day_event|event_country|\n",
      "+----------+------+---------------+-----------------+-----------+-------------+--------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+\n",
      "|  16874807|192759|              1|          1469601|      22742|         1975|                   1|a296494aa7a041|           399863|          87414|             2|             AU>02|          1|        0|           AU|\n",
      "|  16874807|137006|              1|           916403|      17587|          859|                   1|a296494aa7a041|           399863|          87414|             2|             AU>02|          1|        0|           AU|\n",
      "+----------+------+---------------+-----------------+-----------+-------------+--------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_df = spark.read.schema(page_views_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "  .alias('page_views') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_df.createOrReplaceTempView('page_views')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_filter = ''\n",
    "\n",
    "if evaluation:\n",
    "    additional_filter = '''\n",
    "    AND NOT EXISTS (SELECT uuid_event FROM validation_users_docs_to_ignore \n",
    "      WHERE uuid_event = p.uuid_pv\n",
    "      AND document_id_promo = p.document_id_pv)\n",
    "    '''\n",
    "else:\n",
    "    additional_filter = '''\n",
    "    AND NOT EXISTS (SELECT uuid_event FROM test_users_docs_timestamp_to_ignore \n",
    "      WHERE uuid_event = p.uuid_pv\n",
    "      AND document_id_promo = p.document_id_pv\n",
    "      AND p.timestamp_pv >= timestamp_event)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_train_df = spark.sql('''\n",
    "  SELECT * FROM page_views p \n",
    "    WHERE EXISTS (SELECT uuid_event FROM users_to_profile\n",
    "    WHERE uuid_event = p.uuid_pv)                                     \n",
    "  ''' + additional_filter).alias('views') \\\n",
    "  .join(documents_df, on=F.col(\"document_id_pv\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "  .filter('dummyDocumentsEntities is not null OR dummyDocumentsTopics is not null OR dummyDocumentsCategory is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = '../data/outbrain/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_BUCKET_FOLDER+'categories_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(categories_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_BUCKET_FOLDER+'topics_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(topics_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_BUCKET_FOLDER+'entities_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(entities_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else -1, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_by_user_df = page_views_train_df \\\n",
    "  .select(\n",
    "    'uuid_pv', \n",
    "    'document_id_pv', \n",
    "    int_null_to_minus_one_udf('timestamp_pv').alias('timestamp_pv'), \n",
    "    int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "    float_list_null_to_empty_list_udf('cat_confidence_level_list').alias('cat_confidence_level_list'), \n",
    "    int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "    float_list_null_to_empty_list_udf('top_confidence_level_list').alias('top_confidence_level_list'), \n",
    "    str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "    float_list_null_to_empty_list_udf('ent_confidence_level_list').alias('ent_confidence_level_list')) \\\n",
    "  .groupBy('uuid_pv') \\\n",
    "  .agg(F.collect_list('document_id_pv').alias('document_id_pv_list'),\n",
    "    F.collect_list('timestamp_pv').alias('timestamp_pv_list'),\n",
    "    F.collect_list('category_id_list').alias('category_id_lists'),\n",
    "    F.collect_list('cat_confidence_level_list').alias('cat_confidence_level_lists'),\n",
    "    F.collect_list('topic_id_list').alias('topic_id_lists'),\n",
    "    F.collect_list('top_confidence_level_list').alias('top_confidence_level_lists'),\n",
    "    F.collect_list('entity_id_list').alias('entity_id_lists'),\n",
    "    F.collect_list('ent_confidence_level_list').alias('ent_confidence_level_lists'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid_pv: string, document_id_pv_list: array<int>, timestamp_pv_list: array<int>, category_id_lists: array<array<int>>, cat_confidence_level_lists: array<array<float>>, topic_id_lists: array<array<int>>, top_confidence_level_lists: array<array<float>>, entity_id_lists: array<array<string>>, ent_confidence_level_lists: array<array<float>>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_views_by_user_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid_pv: string, document_id_pv_list: array<int>, timestamp_pv_list: array<int>, category_id_lists: array<array<int>>, cat_confidence_level_lists: array<array<float>>, topic_id_lists: array<array<int>>, top_confidence_level_lists: array<array<float>>, entity_id_lists: array<array<string>>, ent_confidence_level_lists: array<array<float>>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_views_by_user_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_aspects(docs_aspects, aspect_docs_counts):\n",
    "    docs_aspects_merged_lists = defaultdict(list)\n",
    "  \n",
    "    for doc_aspects in docs_aspects: #循环每篇文章\n",
    "        for key in doc_aspects.keys(): #遍历每篇文章的主题/类别/实体\n",
    "            docs_aspects_merged_lists[key].append(doc_aspects[key]) # 相同类目聚合\n",
    "      \n",
    "    docs_aspects_stats = {}\n",
    "    for key in docs_aspects_merged_lists.keys():\n",
    "        aspect_list = docs_aspects_merged_lists[key] \n",
    "        tf = len(aspect_list)\n",
    "        idf = math.log(documents_total / float(aspect_docs_counts[key]))\n",
    "    \n",
    "        confid_mean = sum(aspect_list) / float(len(aspect_list))\n",
    "        docs_aspects_stats[key] = [tf*idf, confid_mean]\n",
    "      \n",
    "    return docs_aspects_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_profile(docs_aspects_list, docs_aspects_confidence_list, aspect_docs_counts):\n",
    "    docs_aspects = []\n",
    "    for doc_aspects_list, doc_aspects_confidence_list in zip(docs_aspects_list, docs_aspects_confidence_list):\n",
    "        doc_aspects = dict(zip(doc_aspects_list, doc_aspects_confidence_list))\n",
    "        docs_aspects.append(doc_aspects)\n",
    "      \n",
    "    user_aspects = get_user_aspects(docs_aspects, aspect_docs_counts)\n",
    "    return user_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_len_udf = F.udf(lambda docs_list: len(docs_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_categories_user_profile_map_udf = F.udf(\n",
    "  lambda docs_aspects_list, docs_aspects_confidence_list: \\\n",
    "    generate_user_profile(docs_aspects_list, \n",
    "    docs_aspects_confidence_list, \n",
    "    categories_docs_counts), \n",
    "  MapType(IntegerType(), ArrayType(FloatType()), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_topics_user_profile_map_udf = F.udf(\n",
    "  lambda docs_aspects_list, docs_aspects_confidence_list: \\\n",
    "    generate_user_profile(docs_aspects_list, \n",
    "    docs_aspects_confidence_list, \n",
    "    topics_docs_counts), \n",
    "  MapType(IntegerType(), ArrayType(FloatType()), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_entities_user_profile_map_udf = F.udf(\n",
    "  lambda docs_aspects_list, docs_aspects_confidence_list: \\\n",
    "    generate_user_profile(docs_aspects_list, \n",
    "    docs_aspects_confidence_list, \n",
    "    entities_docs_counts), \n",
    "  MapType(StringType(), ArrayType(FloatType()), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df = page_views_by_user_df \\\n",
    "  .withColumn('views', get_list_len_udf('document_id_pv_list')) \\\n",
    "  .withColumn('categories', generate_categories_user_profile_map_udf('category_id_lists', \n",
    "    'cat_confidence_level_lists')) \\\n",
    "  .withColumn('topics', generate_topics_user_profile_map_udf('topic_id_lists', \n",
    "    'top_confidence_level_lists')) \\\n",
    "  .withColumn('entities', generate_entities_user_profile_map_udf('entity_id_lists', \n",
    "    'ent_confidence_level_lists')) \\\n",
    "  .select(\n",
    "    F.col('uuid_pv').alias('uuid'), \n",
    "    F.col('document_id_pv_list').alias('doc_ids'),\n",
    "    'views', 'categories', 'topics', 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid: string, doc_ids: array<int>, views: int, categories: map<int,array<float>>, topics: map<int,array<float>>, entities: map<string,array<float>>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_profile_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|          uuid|             doc_ids|views|          categories|              topics|            entities|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|1000615e760786|[2959725, 2730005...|    3|[1914 -> [4.85030...|[77 -> [6.0778503...|[753fa42329661c4e...|\n",
      "|10042103b7ff2b|[1415882, 1415309...|    5|[1808 -> [2.51650...|[97 -> [3.4147518...|[023f51d65c5bdb42...|\n",
      "|1007b9cd87526d|[1493650, 1493650...|    3|[1408 -> [8.8686,...|[160 -> [9.604641...|                  []|\n",
      "|100bc3d05f3126|[2668170, 2729482...|    5|[1408 -> [2.9562,...|[65 -> [4.2764845...|[b165150dc5cfdf67...|\n",
      "|10135333f64db3|           [2893318]|    1|[1708 -> [3.04310...|[14 -> [7.055967,...|[97d3c39f93fd28b3...|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_profile_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_profiles'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df.write.parquet('hdfs:/user/lzhao/data/outbrain/preprocessed/'+table_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
