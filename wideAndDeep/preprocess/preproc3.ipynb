{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, TimestampType, FloatType, ArrayType, MapType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = conf.setMaster(\"yarn\")\n",
    "conf = conf.set(\"spark.app.name\", \"recommend-ctr\")\n",
    "conf = conf.set(\"spark.executor.memory\", \"6g\")\n",
    "conf = conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf = conf.set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "conf = conf.set(\"spark.executor.instances\", \"110\")\n",
    "conf = conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_to_unix_epoch(date_time):\n",
    "    return int(time.mktime(date_time.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_to_unix_epoch_treated(dt):\n",
    "    if dt != None:\n",
    "        try:\n",
    "            epoch = date_time_to_unix_epoch(dt)\n",
    "            return epoch\n",
    "        except Exception as e:\n",
    "            print(\"Error processing dt={}\".format(dt), e)\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_null_to_zero_int_udf = F.udf(lambda x: date_time_to_unix_epoch_treated(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT_DEFAULT_NULL_VALUE = -1\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else INT_DEFAULT_NULL_VALUE, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_day_from_timestamp(ts):\n",
    "    return int(ts / 1000 / 60 / 60 / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: truncate_day_from_timestamp(ts), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_country_state_udf = F.udf(lambda geo: geo.strip()[:5] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_len_udf = F.udf(lambda x: len(x) if x != None else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading UTC/BST for each country and US / CA states (local time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_utc_dst_df = pd.read_csv('./data/country_codes_utc_dst_tz_delta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_utc_dst_dict = dict(zip(country_utc_dst_df['country_code'].tolist(), country_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_utc_dst_broad = sc.broadcast(countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_utc_dst_df = pd.read_csv('./data/us_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_utc_dst_dict = dict(zip(us_states_utc_dst_df['state_abb'].tolist(), us_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "us_states_utc_dst_broad = sc.broadcast(us_states_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_states_utc_dst_df = pd.read_csv('./data/ca_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_countries_utc_dst_dict = dict(zip(ca_states_utc_dst_df['state_abb'].tolist(), ca_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "ca_countries_utc_dst_broad = sc.broadcast(ca_countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading competition csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/preprocessed/\"\n",
    "DATA_BUCKET_FOLDER = \"hdfs:/user/lzhao/data/outbrain/\"\n",
    "SPARK_TEMP_FOLDER = \"hdfs:/user/lzhao/data/outbrain/spark-temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_schema = StructType(\n",
    "  [StructField(\"display_id\", IntegerType(), True),\n",
    "  StructField(\"uuid_event\", StringType(), True),                    \n",
    "  StructField(\"document_id_event\", IntegerType(), True),\n",
    "  StructField(\"timestamp_event\", IntegerType(), True),\n",
    "  StructField(\"platform_event\", IntegerType(), True),\n",
    "  StructField(\"geo_location_event\", StringType(), True)]\n",
    "  )\n",
    "\n",
    "events_df = spark.read.schema(events_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "  .withColumn('dummyEvents', F.lit(1)) \\\n",
    "  .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "  .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "  .withColumn('event_country_state', extract_country_state_udf('geo_location_event')) \\\n",
    "  .alias('events') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23120126"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23119786"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with empty \"geo_location\"\n",
    "events_df = events_df.dropna(subset=\"geo_location_event\")\n",
    "events_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23119781"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with empty \"platform\"\n",
    "events_df = events_df.dropna(subset=\"platform_event\")\n",
    "events_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "  [StructField(\"uuid_pv\", StringType(), True),\n",
    "  StructField(\"document_id_pv\", IntegerType(), True),\n",
    "  StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "  StructField(\"platform_pv\", IntegerType(), True),\n",
    "  StructField(\"geo_location_pv\", StringType(), True),\n",
    "  StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "  )\n",
    "\n",
    "page_views_df = spark.read.schema(page_views_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "  .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "  .alias('page_views')        \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|       uuid_pv|document_id_pv|timestamp_pv|platform_pv|geo_location_pv|traffic_source_pv|day_pv|\n",
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|1fd5f051fba643|           120|    31905835|          1|             RS|                2|     0|\n",
      "|8557aa9004be3b|           120|    32053104|          1|          VN>44|                2|     0|\n",
      "|c351b277a358f0|           120|    54013023|          1|          KR>12|                1|     0|\n",
      "|8205775c5387f9|           120|    44196592|          1|          IN>16|                2|     0|\n",
      "|9cb0ccd8458371|           120|    65817371|          1|      US>CA>807|                2|     0|\n",
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_users_df  = spark.sql('''\n",
    "  SELECT uuid_pv, document_id_pv, max(timestamp_pv) as max_timestamp_pv, 1 as dummyPageView\n",
    "  FROM page_views p \n",
    "  GROUP BY uuid_pv, document_id_pv\n",
    "  ''').alias('page_views_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_content_schema = StructType(\n",
    "  [StructField(\"ad_id\", IntegerType(), True),\n",
    "  StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "  StructField(\"campaign_id\", IntegerType(), True),\n",
    "  StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "  )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "  .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "  [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "  StructField(\"source_id\", IntegerType(), True),                    \n",
    "  StructField(\"publisher_id\", IntegerType(), True),\n",
    "  StructField(\"publish_time\", TimestampType(), True)]\n",
    "  )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "  .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with empty \"source_id\"\n",
    "documents_meta_df = documents_meta_df.dropna(subset=\"source_id\")\n",
    "documents_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14394"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_publishers_df = documents_meta_df.select([\"source_id\", \"publisher_id\"]).dropDuplicates()\n",
    "source_publishers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5058"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of source_ids without publisher_id\n",
    "rows_no_pub = source_publishers_df.filter(\"publisher_id is NULL\")\n",
    "source_ids_without_publisher = [row['source_id'] for row in rows_no_pub.collect()]\n",
    "len(source_ids_without_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum value of publisher_id used so far\n",
    "max_pub = max(source_publishers_df.select([\"publisher_id\"]).dropna().collect())['publisher_id']\n",
    "max_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source_id=9376, publisher_id=1264),\n",
       " Row(source_id=9465, publisher_id=1265)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows filled with new publisher_ids\n",
    "new_publishers = [(source, max_pub + 1 + nr) for nr, source in enumerate(source_ids_without_publisher)]\n",
    "new_publishers_df = spark.createDataFrame(new_publishers, (\"source_id\", \"publisher_id\"))\n",
    "new_publishers_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source_id=2555, publisher_id=6317),\n",
       " Row(source_id=422, publisher_id=6318),\n",
       " Row(source_id=5391, publisher_id=6319),\n",
       " Row(source_id=7874, publisher_id=6320),\n",
       " Row(source_id=8054, publisher_id=6321)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old and new publishers merged\n",
    "fixed_source_publishers_df = source_publishers_df.dropna().union(new_publishers_df)\n",
    "fixed_source_publishers_df.collect()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update documents_meta with bew publishers\n",
    "documents_meta_df = documents_meta_df.drop('publisher_id').join(fixed_source_publishers_df, on='source_id')\n",
    "documents_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining with Page Views to get traffic_source_pv\n",
    "events_joined_df = events_df.join(documents_meta_df \\\n",
    "      .withColumnRenamed('source_id', 'source_id_doc_event') \\\n",
    "      .withColumnRenamed('publisher_id', 'publisher_doc_event') \\\n",
    "      .withColumnRenamed('publish_time', 'publish_time_doc_event'),\n",
    "    on=F.col(\"document_id_event\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "  .join(page_views_df, \n",
    "    on=[F.col('uuid_event') == F.col('uuid_pv'),\n",
    "      F.col('document_id_event') == F.col('document_id_pv'),\n",
    "      F.col('platform_event') == F.col('platform_pv'),\n",
    "      F.col('geo_location_event') == F.col('geo_location_pv'),\n",
    "      F.col('day_event') == F.col('day_pv')],\n",
    "    how='left') \\\n",
    "  .alias('events').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "  [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "  StructField(\"category_id\", IntegerType(), True),                    \n",
    "  StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "  )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "  .alias('documents_categories').cache()\n",
    "\n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    " .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "   F.collect_list('confidence_level_cat').alias('confidence_level_cat_list')) \\\n",
    " .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    " .alias('documents_categories_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "  [StructField(\"document_id_top\", IntegerType(), True),\n",
    "  StructField(\"topic_id\", IntegerType(), True),                    \n",
    "  StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "  )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\") \\\n",
    "  .alias('documents_topics').cache()\n",
    "    \n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "  .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "    F.collect_list('confidence_level_top').alias('confidence_level_top_list')) \\\n",
    "  .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "  .alias('documents_topics_grouped') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "  [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "  StructField(\"entity_id\", StringType(), True),                    \n",
    "  StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "  )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\") \\\n",
    "  .alias('documents_entities').cache()\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "  .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "    F.collect_list('confidence_level_ent').alias('confidence_level_ent_list')) \\\n",
    "  .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "  .alias('documents_entities_grouped') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_train_schema = StructType(\n",
    "  [StructField(\"display_id\", IntegerType(), True),\n",
    "  StructField(\"ad_id\", IntegerType(), True),                    \n",
    "  StructField(\"clicked\", IntegerType(), True)]\n",
    "  )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema) \\\n",
    "  .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "  .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "  .withColumn('dummyClicksTrain', F.lit(1)).alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_train_joined_df = clicks_train_df \\\n",
    "  .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "  .join(documents_meta_df, \n",
    "    on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), \n",
    "    how='left') \\\n",
    "  .join(events_joined_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(display_id=148, ad_id=89351, clicked=1, dummyClicksTrain=1, document_id_promo=990613, campaign_id=7617, advertiser_id=2181, dummyPromotedContent=1, source_id=9457, document_id_doc=990613, publish_time=datetime.datetime(2015, 12, 8, 16, 0), dummyDocumentsMeta=1, publisher_id=6123, uuid_event='9adce6a5363308', document_id_event=1205772, timestamp_event=11202, platform_event=2, geo_location_event='US>LA>612', dummyEvents=1, day_event=0, event_country='US', event_country_state='US>LA', source_id_doc_event=9135, document_id_doc=1205772, publish_time_doc_event=datetime.datetime(2016, 3, 29, 1, 0), dummyDocumentsMeta=1, publisher_doc_event=1042, uuid_pv='9adce6a5363308', document_id_pv=1205772, timestamp_pv=11202, platform_pv=2, geo_location_pv='US>LA>612', traffic_source_pv=1, day_pv=0),\n",
       " Row(display_id=148, ad_id=152656, clicked=0, dummyClicksTrain=1, document_id_promo=1086755, campaign_id=10511, advertiser_id=2151, dummyPromotedContent=1, source_id=7654, document_id_doc=1086755, publish_time=datetime.datetime(2015, 11, 16, 16, 0), dummyDocumentsMeta=1, publisher_id=5402, uuid_event='9adce6a5363308', document_id_event=1205772, timestamp_event=11202, platform_event=2, geo_location_event='US>LA>612', dummyEvents=1, day_event=0, event_country='US', event_country_state='US>LA', source_id_doc_event=9135, document_id_doc=1205772, publish_time_doc_event=datetime.datetime(2016, 3, 29, 1, 0), dummyDocumentsMeta=1, publisher_doc_event=1042, uuid_pv='9adce6a5363308', document_id_pv=1205772, timestamp_pv=11202, platform_pv=2, geo_location_pv='US>LA>612', traffic_source_pv=1, day_pv=0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_train_joined_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+table_name) \\\n",
    "  .withColumn('dummyUserProfiles', F.lit(1)).alias('user_profiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Train/validation set | Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_df.count() = 59761827\n"
     ]
    }
   ],
   "source": [
    "if evaluation:       \n",
    "    validation_set_exported_df = spark.read.parquet(\n",
    "    OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "  .alias('validation_set') \n",
    "          \n",
    "    validation_set_exported_df.select('display_id').distinct() \\\n",
    "    .createOrReplaceTempView(\"validation_display_ids\")\n",
    "  \n",
    "  \n",
    "    validation_set_df = spark.sql('''\n",
    "      SELECT * FROM clicks_train_joined t \n",
    "      WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "      WHERE display_id = t.display_id)''').alias('clicks') \\\n",
    "    .join(documents_categories_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), \n",
    "      how='left') \\\n",
    "    .join(documents_topics_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), \n",
    "      how='left') \\\n",
    "    .join(documents_entities_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), \n",
    "      how='left') \\\n",
    "    .join(documents_categories_grouped_df \\\n",
    "        .withColumnRenamed('category_id_list', 'doc_event_category_id_list') \\\n",
    "        .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "        .alias('documents_event_categories_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "      how='left') \\\n",
    "    .join(documents_topics_grouped_df \\\n",
    "        .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "        .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "        .alias('documents_event_topics_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "      how='left') \\\n",
    "    .join(documents_entities_grouped_df \\\n",
    "        .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "        .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "        .alias('documents_event_entities_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "      how='left') \\\n",
    "    .join(page_views_users_df, \n",
    "      on=[F.col(\"clicks.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "        F.col(\"clicks.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "      how='left')\n",
    "  \n",
    "  #print(\"validation_set_df.count() =\", validation_set_df.count())\n",
    "      \n",
    "  #Added to validation set information about the event and the user for statistics of the error (avg ctr)\n",
    "    validation_set_ground_truth_df = validation_set_df.filter('clicked = 1') \\\n",
    "    .join(user_profiles_df, \n",
    "      on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], \n",
    "      how='left') \\\n",
    "    .withColumn('user_categories_count', list_len_udf('category_id_list')) \\\n",
    "    .withColumn('user_topics_count', list_len_udf('topic_id_list')) \\\n",
    "    .withColumn('user_entities_count', list_len_udf('entity_id_list')) \\\n",
    "    .select('display_id','ad_id','platform_event', 'day_event', 'timestamp_event', \n",
    "      'geo_location_event', 'event_country', 'event_country_state', 'views',\n",
    "      'user_categories_count', 'user_topics_count', 'user_entities_count') \\\n",
    "    .withColumnRenamed('ad_id','ad_id_gt') \\\n",
    "    .withColumnRenamed('views','user_views_count') \\\n",
    "    .cache()\n",
    "  #print(\"validation_set_ground_truth_df.count() =\", validation_set_ground_truth_df.count())\n",
    "  \n",
    "    train_set_df = spark.sql('''\n",
    "    SELECT * FROM clicks_train_joined t \n",
    "    WHERE NOT EXISTS (SELECT display_id FROM validation_display_ids \n",
    "    WHERE display_id = t.display_id)''').cache()\n",
    "    print(\"train_set_df.count() =\", train_set_df.count())\n",
    "    \n",
    "else:\n",
    "    \n",
    "    clicks_test_schema = StructType(\n",
    "    [StructField(\"display_id\", IntegerType(), True),\n",
    "    StructField(\"ad_id\", IntegerType(), True)]\n",
    "    )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"clicks_test.csv\") \\\n",
    "    .withColumn('dummyClicksTest', F.lit(1)) \\\n",
    "    .withColumn('clicked', F.lit(-999)) \\\n",
    "    .alias('clicks_test')\n",
    "      \n",
    "      \n",
    "    test_set_df = clicks_test_df \\\n",
    "    .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "    .join(documents_meta_df, \n",
    "      on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"),\n",
    "      how='left') \\\n",
    "    .join(documents_categories_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), \n",
    "      how='left') \\\n",
    "    .join(documents_topics_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), \n",
    "      how='left') \\\n",
    "    .join(documents_entities_grouped_df, \n",
    "      on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), \n",
    "      how='left') \\\n",
    "    .join(events_joined_df, on='display_id', how='left') \\\n",
    "    .join(documents_categories_grouped_df \\\n",
    "        .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "        .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "        .alias('documents_event_categories_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "      how='left') \\\n",
    "    .join(documents_topics_grouped_df \\\n",
    "        .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "        .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "        .alias('documents_event_topics_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "      how='left') \\\n",
    "    .join(documents_entities_grouped_df \\\n",
    "        .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "        .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "        .alias('documents_event_entities_grouped'), \n",
    "      on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "      how='left') \\\n",
    "    .join(page_views_users_df, \n",
    "      on=[F.col(\"events.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "        F.col(\"promoted_content.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "      how='left')\n",
    "\n",
    "    train_set_df = clicks_train_joined_df.cache() \n",
    "    print(\"train_set_df.count() =\", train_set_df.count())       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_null(value):\n",
    "    return value is None or len(str(value).strip()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESS_SPECIAL_CAT_VALUE = 'less'\n",
    "def get_category_field_values_counts(field, df, min_threshold=10):\n",
    "    category_counts = dict(list(filter(lambda x: not is_null(x[0]) and x[1] >= min_threshold, df.select(field).groupBy(field).count().rdd.map(lambda x: (x[0], x[1])).collect())))\n",
    "    #Adding a special value to create a feature for values in this category that are less than min_threshold \n",
    "    category_counts[LESS_SPECIAL_CAT_VALUE] = -1\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building category values counters and indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_values_counts = get_category_field_values_counts('event_country', events_df, min_threshold=10)\n",
    "len(event_country_values_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_state_values_counts = get_category_field_values_counts('event_country_state', events_df, min_threshold=10)\n",
    "len(event_country_state_values_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_geo_location_values_counts = get_category_field_values_counts('geo_location_event', events_df, min_threshold=10)\n",
    "len(event_geo_location_values_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52439"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_entity_id_values_counts = get_category_field_values_counts('entity_id', documents_entities_df, min_threshold=10)\n",
    "len(doc_entity_id_values_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing average CTR by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentiles(df, field, quantiles_levels=None, max_error_rate=0.0):\n",
    "    if quantiles_levels is None:\n",
    "        quantiles_levels = np.arange(0.0, 1.1, 0.1).tolist() \n",
    "    quantiles = df.approxQuantile(field, quantiles_levels, max_error_rate)\n",
    "    return dict(zip(quantiles_levels, quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REG = 10\n",
    "REG = 0\n",
    "ctr_udf = F.udf(lambda clicks, views: clicks / float(views + REG), FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_id_popularity_df = train_set_df.groupby('ad_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_id_popularity = ad_id_popularity_df.filter('views > 5').select('ad_id', 'ctr', 'views') \\\n",
    "  .rdd.map(lambda x: (x['ad_id'], (x['ctr'], x['views'], 1, 1))).collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_id_popularity_broad = sc.broadcast(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06668111681938171, 13842, 1, 1),\n",
       " (0.3735618591308594, 33550, 1, 1),\n",
       " (0.10664335638284683, 572, 1, 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ad_id_popularity.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192108"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1552832145956732"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_avg_ctr = sum(map(lambda x: x[0], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1940534152412439"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], ad_id_popularity.values())) / float(sum(map(lambda x: x[1], ad_id_popularity.values())))\n",
    "ad_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_median = np.median(np.array(list(map(lambda x: x[1], ad_id_popularity.values()))))\n",
    "ad_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.5930986736627"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_mean = sum(map(lambda x: x[1], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by document_id (promoted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_popularity_df = train_set_df \\\n",
    "  .groupby('document_id_promo') \\\n",
    "  .agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_popularity = document_id_popularity_df.filter('views > 5') \\\n",
    "  .select('document_id_promo', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['document_id_promo'], \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74767"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_id_popularity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_popularity_broad = sc.broadcast(document_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1504891450153877"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_avg_ctr = sum(map(lambda x: x[0], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19380680502581238"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_weighted_avg_ctr = sum(list(map(lambda x: x[0]*x[1], document_id_popularity.values()))) / float(sum(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_median = np.median(np.array(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797.3909746278439"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_mean = sum(map(lambda x: x[1], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average CTR by (doc_event, doc_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr_df = train_set_df.groupBy('document_id_event', 'document_id_promo') \\\n",
    "  .agg(F.sum('clicked').alias('clicks'), \n",
    "    F.count('*').alias('views'), F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr = doc_event_doc_ad_avg_ctr_df.filter('views > 5') \\\n",
    "  .select('document_id_event', 'document_id_promo','ctr', 'views', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: ((x['document_id_event'], x['document_id_promo']), \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302456"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr_broad = sc.broadcast(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by country, source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_by_country_popularity_df = train_set_df \\\n",
    "  .select('clicked', 'source_id', 'event_country', 'ad_id') \\\n",
    "  .groupby('event_country', 'source_id') \\\n",
    "  .agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29856"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n",
    "source_id_by_country_popularity = source_id_by_country_popularity_df.filter('views > 5 and source_id is not null and event_country <> \"\"').select('event_country', 'source_id', 'ctr', 'views', 'distinct_ad_ids')         .rdd.map(lambda x: ((x['event_country'], x['source_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_by_country_popularity)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_by_country_popularity_broad = sc.broadcast(source_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1860297896776317"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_avg_ctr = sum(map(lambda x: x[0], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19364919644870532"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], source_id_by_country_popularity.values())) / float(sum(map(lambda x: x[1], source_id_by_country_popularity.values())))\n",
    "source_id_by_country_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_median = np.median(np.array(list(map(lambda x: x[1], source_id_by_country_popularity.values()))))\n",
    "source_id_by_country_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999.1519962486602"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_mean = sum(map(lambda x: x[1], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_popularity_df = train_set_df.select('clicked', 'source_id', 'ad_id') \\\n",
    "  .groupby('source_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_popularity = source_id_popularity_df \\\n",
    "  .filter('views > 10 and source_id is not null') \\\n",
    "  .select('source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['source_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))) \\\n",
    "  .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5628"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_popularity_broad = sc.broadcast(source_id_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by publisher_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_popularity_df = train_set_df.select('clicked', 'publisher_id', 'ad_id') \\\n",
    "  .groupby('publisher_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_popularity = publisher_popularity_df \\\n",
    "  .filter('views > 10 and publisher_id is not null') \\\n",
    "  .select('publisher_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['publisher_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))) \\\n",
    "  .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4671"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publisher_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_popularity_broad = sc.broadcast(publisher_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertiser_id_popularity_df = train_set_df.select('clicked', 'advertiser_id', 'ad_id') \\\n",
    "  .groupby('advertiser_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertiser_id_popularity = advertiser_id_popularity_df \\\n",
    "  .filter('views > 10 and advertiser_id is not null') \\\n",
    "  .select('advertiser_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "    .rdd.map(lambda x: (x['advertiser_id'], \n",
    "      (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertiser_id_popularity_broad = sc.broadcast(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by campaign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id_popularity_df = train_set_df.select('clicked', 'campaign_id', 'ad_id') \\\n",
    "  .groupby('campaign_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id_popularity = campaign_id_popularity_df \\\n",
    "  .filter('views > 10 and campaign_id is not null') \\\n",
    "  .select('campaign_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['campaign_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))) \\\n",
    "  .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25270"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(campaign_id_popularity)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id_popularity_broad = sc.broadcast(campaign_id_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_popularity_df = train_set_df.join(\n",
    "    documents_categories_df.alias('cat_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "  .select('clicked', 'category_id', 'confidence_level_cat', 'ad_id') \\\n",
    "  .groupby('category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "    F.count('*').alias('views'),\n",
    "    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_popularity = category_id_popularity_df.filter('views > 10') \\\n",
    "  .select('category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['category_id'], \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_popularity_broad = sc.broadcast(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24857684969902039, 29512, 261, 0.2659369122112505),\n",
       " (0.1463375687599182, 306367, 1864, 0.3836574482880412),\n",
       " (0.2693001329898834, 1864184, 15180, 0.6951355794027785),\n",
       " (0.2103358805179596, 2178991, 16646, 0.5190177568460855),\n",
       " (0.1664503812789917, 167990, 2003, 0.07435643983049306)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_popularity.values())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692507.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(list(map(lambda x: x[1], category_id_popularity.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246361.957894737"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], category_id_popularity.values())) / float(len(category_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_by_country_popularity_df = train_set_df \\\n",
    "  .join(documents_categories_df.alias('cat_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "  .select('clicked', 'category_id', 'confidence_level_cat', 'event_country', 'ad_id') \\\n",
    "  .groupby('event_country','category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "    F.count('*').alias('views'), \n",
    "    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_by_country_popularity = category_id_by_country_popularity_df \\\n",
    "  .filter('views > 10 and event_country <> \"\"') \\\n",
    "  .select('event_country', 'category_id', 'ctr', 'views', 'avg_confidence_level_cat', \n",
    "    'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: ((x['event_country'], x['category_id']), \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10987"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_by_country_popularity_broad = sc.broadcast(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_popularity_df = train_set_df.join(\n",
    "    documents_topics_df.alias('top_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "  .select('clicked', 'topic_id', 'confidence_level_top', 'ad_id') \\\n",
    "  .groupby('topic_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_popularity = topic_id_popularity_df.filter('views > 10') \\\n",
    "  .select('topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['topic_id'], \\\n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_popularity_broad = sc.broadcast(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526644.25"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998840613.206667"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[2]*x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_by_country_popularity_df = train_set_df.join(\n",
    "    documents_topics_df.alias('top_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "  .select('clicked', 'topic_id', 'confidence_level_top','event_country', 'ad_id') \\\n",
    "  .groupby('event_country','topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "    F.count('*').alias('views'), \n",
    "    F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_id_by_country_popularity = topic_id_by_country_popularity_df \\\n",
    "  .filter('views > 10 and event_country <> \"\"') \\\n",
    "  .select('event_country', 'topic_id', 'ctr', 'views', 'avg_confidence_level_top', \n",
    "    'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: ((x['event_country'], x['topic_id']), \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33071"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_id_id_by_country_popularity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_id_by_country_popularity_broad = sc.broadcast(topic_id_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_popularity_df = train_set_df.join(\n",
    "    documents_entities_df.alias('ent_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "  .select('clicked', 'entity_id', 'confidence_level_ent', 'ad_id') \\\n",
    "  .groupby('entity_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views'),\n",
    "    F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_popularity = entity_id_popularity_df.filter('views > 5') \\\n",
    "  .select('entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: (x['entity_id'], \n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78120"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_popularity_broad = sc.broadcast(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(list(map(lambda x: x[1], entity_id_popularity.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915.9886584741423"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], entity_id_popularity.values())) / float(len(entity_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CTR by (country, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity_df = train_set_df.join(\n",
    "    documents_entities_df.alias('ent_local'), \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "  .select('clicked', 'entity_id', 'event_country', 'confidence_level_ent','ad_id') \\\n",
    "  .groupby('event_country','entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "    F.count('*').alias('views'),\n",
    "    F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "  .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity = entity_id_by_country_popularity_df \\\n",
    "  .filter('views > 5 and event_country <> \"\"') \\\n",
    "  .select('event_country', 'entity_id', 'ctr', 'views', 'avg_confidence_level_ent', \n",
    "    'distinct_ad_ids') \\\n",
    "  .rdd.map(lambda x: ((x['event_country'], x['entity_id']),\n",
    "    (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217703"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_id_by_country_popularity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity_broad = sc.broadcast(entity_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/outbrain/preprocessed'+'categories_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    categories_docs_counts = cPickle.load(input_file)    \n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/outbrain/preprocessed'+'topics_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    topics_docs_counts = cPickle.load(input_file)\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/outbrain/preprocessed'+'entities_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    entities_docs_counts = cPickle.load(input_file)\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996816"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Publish Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_times_df = train_set_df.filter('publish_time is not null') \\\n",
    ".select('document_id_promo','publish_time') \\\n",
    ".distinct().select(F.col('publish_time').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|publish_time|\n",
      "+------------+\n",
      "|  1435593600|\n",
      "|  1441814400|\n",
      "|  1458144000|\n",
      "|  1464710400|\n",
      "|  1446912000|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "publish_times_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_time_percentiles = get_percentiles(publish_times_df, 'publish_time', quantiles_levels=[0.5], max_error_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 1464105600.0}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_time_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_time_median = int(publish_time_percentiles[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 5, 24, 16, 0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(publish_time_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_diff(newer_timestamp, older_timestamp):\n",
    "    sec_diff = newer_timestamp - older_timestamp\n",
    "    days_diff = sec_diff / 60 / 60 / 24\n",
    "    return days_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to think\n",
    "def get_time_decay_factor(timestamp, timestamp_ref=None, alpha=0.001):\n",
    "    if timestamp_ref is None:\n",
    "        timestamp_ref = time.time()\n",
    "        \n",
    "    days_diff = get_days_diff(timestamp_ref, timestamp)\n",
    "    denominator = math.pow(1+alpha, days_diff)\n",
    "    if denominator != 0:\n",
    "        return 1.0 / denominator\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DECAY_ALPHA = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dates = [\n",
    "                1476714880, # 7 days\n",
    "                1474727680, # 30 days\n",
    "                1469370880, # 90 days\n",
    "                1461508480,  # 180 days\n",
    "                1445697280, # 1 year\n",
    "                1414161280 # 2 years\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-17 14:34:40 0.5092478658812107\n",
      "2016-09-24 14:34:40 0.503426507591573\n",
      "2016-07-24 14:34:40 0.48806348316584197\n",
      "2016-04-24 14:34:40 0.46635952850625007\n",
      "2015-10-24 14:34:40 0.42559139718160316\n",
      "2014-10-24 14:34:40 0.35461239015004703\n"
     ]
    }
   ],
   "source": [
    "for d in ref_dates:\n",
    "    print(datetime.datetime.utcfromtimestamp(d), get_time_decay_factor(d, alpha=TIME_DECAY_ALPHA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TZ_EST = -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_utc_bst_tz(event_country, event_country_state):\n",
    "    local_tz = DEFAULT_TZ_EST\n",
    "    if len(event_country) > 0:\n",
    "        if event_country in countries_utc_dst_broad.value:\n",
    "            local_tz = countries_utc_dst_broad.value[event_country]\n",
    "            if len(event_country_state)>2:\n",
    "                state = event_country_state[3:5]\n",
    "                if event_country == 'US':  \n",
    "                    if state in us_states_utc_dst_broad.value:\n",
    "                        local_tz = us_states_utc_dst_broad.value[state]                \n",
    "                elif event_country == 'CA':\n",
    "                    if state in ca_countries_utc_dst_broad.value:\n",
    "                        local_tz = ca_countries_utc_dst_broad.value[state] \n",
    "    return float(local_tz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_utc_bst_tz('US', 'US>WV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_bins_dict = {'EARLY_MORNING': 0,\n",
    "             'MORNING': 1,\n",
    "             'MIDDAY': 2,\n",
    "             'AFTERNOON': 3,\n",
    "             'EVENING': 4,\n",
    "             'NIGHT': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_bins_values = sorted(hour_bins_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour_bin(hour):\n",
    "    if hour >= 5 and hour < 8:\n",
    "        hour_bin = hour_bins_dict['EARLY_MORNING']\n",
    "    elif hour >= 8 and hour < 11:\n",
    "        hour_bin = hour_bins_dict['MORNING']\n",
    "    elif hour >= 11 and hour < 14:\n",
    "        hour_bin = hour_bins_dict['MIDDAY']\n",
    "    elif hour >= 14 and hour < 19:\n",
    "        hour_bin = hour_bins_dict['AFTERNOON']\n",
    "    elif hour >= 19 and hour < 22:\n",
    "        hour_bin = hour_bins_dict['EVENING']\n",
    "    else:\n",
    "        hour_bin = hour_bins_dict['NIGHT']\n",
    "    return hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_datetime(dt, event_country, event_country_state):\n",
    "    local_tz = get_local_utc_bst_tz(event_country, event_country_state)  \n",
    "    tz_delta = local_tz - DEFAULT_TZ_EST\n",
    "    local_time = dt +  datetime.timedelta(hours=tz_delta)\n",
    "    return local_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 6, 28, 11, 3, 57, 913950)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_datetime(datetime.datetime.now(), 'US', 'US>CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(dt):\n",
    "    return dt.weekday() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_ref = date_time_to_unix_epoch(datetime.datetime(2016, 6, 29, 3, 59, 59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1467172799"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_factor_default = get_time_decay_factor(publish_time_median, timestamp_ref, alpha=TIME_DECAY_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay_factor_default: 0.98241096698168\n"
     ]
    }
   ],
   "source": [
    "print(\"decay_factor_default:\", decay_factor_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to think\n",
    "def get_confidence_sample_size(sample, max_for_reference=100000):\n",
    "    #Avoiding overflow for large sample size\n",
    "    if sample >= max_for_reference:\n",
    "        return 1.0\n",
    "\n",
    "    ref_log = math.log(1+max_for_reference, 2) #Curiosly reference in log  with base 2 gives a slightly higher score, so I will keep\n",
    "    \n",
    "    return math.log(1+sample) / float(ref_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "0.5 0.024411410743763327\n",
      "1 0.041731582304281624\n",
      "2 0.06614299304804495\n",
      "3 0.08346316460856325\n",
      "4 0.09689773339641579\n",
      "5 0.10787457535232657\n",
      "10 0.14436755531919657\n",
      "20 0.183298356035222\n",
      "30 0.20674645107847822\n",
      "100 0.2778577004917695\n",
      "200 0.3192904933647466\n",
      "300 0.34360197720285013\n",
      "1000 0.41594812296601125\n",
      "2000 0.4576496248565576\n",
      "3000 0.48205100545505175\n",
      "10000 0.5545232830964639\n",
      "20000 0.5962518553291584\n",
      "30000 0.6206622626822822\n",
      "50000 0.6514162003061013\n",
      "90000 0.6868039178501281\n",
      "100000 1.0\n",
      "500000 1.0\n",
      "900000 1.0\n",
      "1000000 1.0\n",
      "2171607 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in [0,0.5,1,2,3,4,5,10,20,30,100,200,300,1000,2000,3000,10000,20000,30000, 50000, 90000, 100000, 500000, 900000, 1000000, 2171607]:\n",
    "    print(i, get_confidence_sample_size(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity(an_id, a_dict):\n",
    "    return (a_dict[an_id][0], get_confidence_sample_size(a_dict[an_id][1] / float(a_dict[an_id][2])) * a_dict[an_id][3]) if an_id in a_dict else (None, None)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0833333358168602, 168, 1, 1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_popularity_broad.value[155510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0833333358168602, 0.3088504093192754)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_popularity(155510, ad_id_popularity_broad.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_avg_popularity_from_list(ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity(an_id, pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    #print(\"pops\",pops)\n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_avg_country_popularity_from_list(event_country, ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity((event_country, an_id), pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    \n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "      publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "      category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "      topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "      entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "      output_detailed_list=False):\n",
    "    probs = []\n",
    "    \n",
    "    avg_ctr, confidence = get_popularity(ad_id, ad_id_popularity_broad.value)    \n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_ad_id', avg_ctr, confidence))\n",
    "        \n",
    "    avg_ctr, confidence = get_popularity(document_id, document_id_popularity_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_document_id', avg_ctr, confidence))  \n",
    "        \n",
    "    avg_ctr, confidence = get_popularity((document_id_event, document_id), doc_event_doc_ad_avg_ctr_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_doc_event_doc_ad', avg_ctr, confidence))\n",
    "        \n",
    "        \n",
    "    if source_id != -1:\n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_popularity((event_country, source_id), source_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_popularity(source_id, source_id_popularity_broad.value)        \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id', avg_ctr, confidence))\n",
    "            \n",
    "            \n",
    "    if publisher_id != None:\n",
    "        avg_ctr, confidence = get_popularity(publisher_id, publisher_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_publisher_id', avg_ctr, confidence)) \n",
    "            \n",
    "    if advertiser_id != None:\n",
    "        avg_ctr, confidence = get_popularity(advertiser_id, advertiser_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_advertiser_id', avg_ctr, confidence)) \n",
    "    \n",
    "    if campaign_id != None:\n",
    "        avg_ctr, confidence = get_popularity(campaign_id, campaign_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_campain_id', avg_ctr, confidence))  \n",
    "\n",
    "    if len(entity_ids_by_doc) > 0: \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(\n",
    "              event_country, entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "              entity_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(\n",
    "          entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "          entity_id_popularity_broad.value) \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id', avg_ctr, confidence))\n",
    "            \n",
    "    \n",
    "    \n",
    "    if len(topic_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(\n",
    "              event_country, topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "              topic_id_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(\n",
    "          topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "          topic_id_popularity_broad.value)            \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id', avg_ctr, confidence))\n",
    "    \n",
    "    \n",
    "    if len(category_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(\n",
    "              event_country, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "              category_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id_country', avg_ctr, confidence))\n",
    "        \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(\n",
    "          category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "          category_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id', avg_ctr, confidence))\n",
    "    \n",
    "    #print(\"[get_popularity_score] probs\", probs)\n",
    "    if output_detailed_list:\n",
    "        return probs\n",
    "    \n",
    "    else:    \n",
    "        if len(probs) > 0:\n",
    "            #weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] *  math.log(1+x[2],2), probs)) / float(sum(map(lambda x: math.log(1+x[2],2), probs)))        \n",
    "            weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] * x[2], probs)) / float(sum(map(lambda x: x[2], probs)))                \n",
    "            confidence = max(map(lambda x: x[2], probs))\n",
    "            return weighted_avg_probs_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_dicts(dict1, dict2):\n",
    "    dict1_norm = math.sqrt(sum([v**2 for v in dict1.values()]))\n",
    "    dict2_norm = math.sqrt(sum([v**2 for v in dict2.values()]))\n",
    "    \n",
    "    sum_common_aspects = 0.0\n",
    "    intersections = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            sum_common_aspects += dict1[key] * dict2[key] \n",
    "            intersections += 1\n",
    "        \n",
    "    return sum_common_aspects / (dict1_norm * dict2_norm), intersections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_user_docs_aspects(user_aspect_profile, doc_aspect_ids, doc_aspects_confidence, aspect_docs_counts):\n",
    "    if user_aspect_profile is None or len(user_aspect_profile) == 0 or doc_aspect_ids is None or len(doc_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_aspects = dict(zip(doc_aspect_ids, doc_aspects_confidence))\n",
    "    doc_aspects_tfidf_confid = {}\n",
    "    for key in doc_aspects:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_aspects[key]\n",
    "        doc_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    user_aspects_tfidf_confid = {}    \n",
    "    for key in user_aspect_profile:\n",
    "        tfidf = user_aspect_profile[key][0]\n",
    "        confidence = user_aspect_profile[key][1]\n",
    "        user_aspects_tfidf_confid[key] = tfidf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_aspects_tfidf_confid, user_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_aspects) / float(len(aspect_docs_counts)), \n",
    "          intersections) * math.pow(len(user_aspect_profile) / float(len(aspect_docs_counts)), \n",
    "          intersections)\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_aspects) / float(len(aspect_docs_counts))) * \n",
    "          (len(user_aspect_profile) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_doc_event_doc_ad_aspects(doc_event_aspect_ids, doc_event_aspects_confidence, \n",
    "        doc_ad_aspect_ids, doc_ad_aspects_confidence, \n",
    "        aspect_docs_counts):\n",
    "    if doc_event_aspect_ids is None or len(doc_event_aspect_ids) == 0 \\\n",
    "            or doc_ad_aspect_ids is None or len(doc_ad_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_event_aspects = dict(zip(doc_event_aspect_ids, doc_event_aspects_confidence))\n",
    "    doc_event_aspects_tfidf_confid = {}\n",
    "    for key in doc_event_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_event_aspects[key]\n",
    "        doc_event_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    doc_ad_aspects = dict(zip(doc_ad_aspect_ids, doc_ad_aspects_confidence))\n",
    "    doc_ad_aspects_tfidf_confid = {}\n",
    "    for key in doc_ad_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_ad_aspects[key]\n",
    "        doc_ad_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_event_aspects_tfidf_confid, doc_ad_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_event_aspect_ids) / float(len(aspect_docs_counts)), \n",
    "            intersections) * math.pow(len(doc_ad_aspect_ids) / float(len(aspect_docs_counts)), \n",
    "            intersections)\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_event_aspect_ids) / float(len(aspect_docs_counts))) * \n",
    "          (len(doc_ad_aspect_ids) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "        timestamp_event, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "        entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "        output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_user_docs_aspects(user_categories, category_ids_by_doc, cat_confidence_level_by_doc, categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_user_docs_aspects(user_topics, topic_ids_by_doc, top_confidence_level_by_doc, topics_docs_counts)\n",
    "    if topics_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "    \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_user_docs_aspects(user_entities, entity_ids_by_doc, ent_confidence_level_by_doc, entities_docs_counts)\n",
    "    if entities_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_event_doc_ad_cb_similarity_score(doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "        doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "        doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "        doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "        doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "        doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "        output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    sims = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "        doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "        doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "        categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "        doc_event_topic_ids, doc_event_top_confidence_levels, \n",
    "        doc_ad_topic_ids, doc_ad_top_confidence_levels, \n",
    "        topics_docs_counts)\n",
    "    \n",
    "    if topics_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "        \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "        doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "        doc_ad_entity_ids, doc_ad_ent_confidence_levels, \n",
    "        entities_docs_counts)\n",
    "    \n",
    "    if entities_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_feature_names = ['event_weekend',\n",
    "                      'user_has_already_viewed_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_feature_names = ['user_views',\n",
    "                    'ad_views',\n",
    "                    'doc_views',\n",
    "                    'doc_event_days_since_published',\n",
    "                    'doc_event_hour',\n",
    "                    'doc_ad_days_since_published', \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_feature_names = [                                \n",
    "                'pop_ad_id',       \n",
    "                'pop_ad_id_conf',   \n",
    "                'pop_ad_id_conf_multipl', \n",
    "                'pop_document_id',                \n",
    "                'pop_document_id_conf',\n",
    "                'pop_document_id_conf_multipl',\n",
    "                'pop_publisher_id',\n",
    "                'pop_publisher_id_conf',\n",
    "                'pop_publisher_id_conf_multipl',\n",
    "                'pop_advertiser_id',\n",
    "                'pop_advertiser_id_conf',\n",
    "                'pop_advertiser_id_conf_multipl',\n",
    "                'pop_campain_id',\n",
    "                'pop_campain_id_conf',\n",
    "                'pop_campain_id_conf_multipl',\n",
    "                'pop_doc_event_doc_ad',\n",
    "                'pop_doc_event_doc_ad_conf',\n",
    "                'pop_doc_event_doc_ad_conf_multipl',\n",
    "                'pop_source_id',  \n",
    "                'pop_source_id_conf',\n",
    "                'pop_source_id_conf_multipl',\n",
    "                'pop_source_id_country',\n",
    "                'pop_source_id_country_conf',\n",
    "                'pop_source_id_country_conf_multipl',\n",
    "                'pop_entity_id',    \n",
    "                'pop_entity_id_conf',\n",
    "                'pop_entity_id_conf_multipl',\n",
    "                'pop_entity_id_country',\n",
    "                'pop_entity_id_country_conf',\n",
    "                'pop_entity_id_country_conf_multipl',\n",
    "                'pop_topic_id', \n",
    "                'pop_topic_id_conf',\n",
    "                'pop_topic_id_conf_multipl',\n",
    "                'pop_topic_id_country',\n",
    "                'pop_topic_id_country_conf',\n",
    "                'pop_topic_id_country_conf_multipl',\n",
    "                'pop_category_id', \n",
    "                'pop_category_id_conf',\n",
    "                'pop_category_id_conf_multipl',\n",
    "                'pop_category_id_country',\n",
    "                'pop_category_id_country_conf',\n",
    "                'pop_category_id_country_conf_multipl',\n",
    "                'user_doc_ad_sim_categories',    \n",
    "                'user_doc_ad_sim_categories_conf',\n",
    "                'user_doc_ad_sim_categories_conf_multipl',\n",
    "                'user_doc_ad_sim_topics',    \n",
    "                'user_doc_ad_sim_topics_conf',\n",
    "                'user_doc_ad_sim_topics_conf_multipl',\n",
    "                'user_doc_ad_sim_entities',                    \n",
    "                'user_doc_ad_sim_entities_conf',\n",
    "                'user_doc_ad_sim_entities_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_categories',    \n",
    "                'doc_event_doc_ad_sim_categories_conf',\n",
    "                'doc_event_doc_ad_sim_categories_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_topics',    \n",
    "                'doc_event_doc_ad_sim_topics_conf',\n",
    "                'doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_entities',                    \n",
    "                'doc_event_doc_ad_sim_entities_conf',\n",
    "                'doc_event_doc_ad_sim_entities_conf_multipl'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAFFIC_SOURCE_FV='traffic_source'\n",
    "EVENT_HOUR_FV='event_hour'\n",
    "EVENT_COUNTRY_FV = 'event_country'\n",
    "EVENT_COUNTRY_STATE_FV = 'event_country_state'\n",
    "EVENT_GEO_LOCATION_FV = 'event_geo_location'\n",
    "EVENT_PLATFORM_FV = 'event_platform'\n",
    "AD_ADVERTISER_FV = 'ad_advertiser'\n",
    "DOC_AD_SOURCE_ID_FV='doc_ad_source_id'\n",
    "DOC_AD_PUBLISHER_ID_FV='doc_ad_publisher_id'\n",
    "DOC_EVENT_SOURCE_ID_FV='doc_event_source_id'\n",
    "DOC_EVENT_PUBLISHER_ID_FV='doc_event_publisher_id'\n",
    "DOC_AD_CATEGORY_ID_FV='doc_ad_category_id'\n",
    "DOC_AD_TOPIC_ID_FV='doc_ad_topic_id'\n",
    "DOC_AD_ENTITY_ID_FV='doc_ad_entity_id'\n",
    "DOC_EVENT_CATEGORY_ID_FV='doc_event_category_id'\n",
    "DOC_EVENT_TOPIC_ID_FV='doc_event_topic_id'\n",
    "DOC_EVENT_ENTITY_ID_FV='doc_event_entity_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_feature_names_integral = ['ad_advertiser',\n",
    " 'doc_ad_category_id_1',\n",
    " 'doc_ad_category_id_2',\n",
    " 'doc_ad_category_id_3',\n",
    " 'doc_ad_topic_id_1',\n",
    " 'doc_ad_topic_id_2',\n",
    " 'doc_ad_topic_id_3',\n",
    " 'doc_ad_entity_id_1', \n",
    " 'doc_ad_entity_id_2', \n",
    " 'doc_ad_entity_id_3', \n",
    " 'doc_ad_entity_id_4', \n",
    " 'doc_ad_entity_id_5', \n",
    " 'doc_ad_entity_id_6', \n",
    " 'doc_ad_publisher_id',\n",
    " 'doc_ad_source_id', \n",
    " 'doc_event_category_id_1',\n",
    " 'doc_event_category_id_2',\n",
    " 'doc_event_category_id_3',\n",
    " 'doc_event_topic_id_1',\n",
    " 'doc_event_topic_id_2',\n",
    " 'doc_event_topic_id_3',\n",
    " 'doc_event_entity_id_1',\n",
    " 'doc_event_entity_id_2',\n",
    " 'doc_event_entity_id_3',\n",
    " 'doc_event_entity_id_4',\n",
    " 'doc_event_entity_id_5',\n",
    " 'doc_event_entity_id_6',\n",
    " 'doc_event_publisher_id',\n",
    " 'doc_event_source_id', \n",
    " 'event_country',\n",
    " 'event_country_state',\n",
    " 'event_geo_location',\n",
    " 'event_hour',\n",
    " 'event_platform',\n",
    " 'traffic_source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_labels_integral = bool_feature_names + int_feature_names + float_feature_names + category_feature_names_integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_labels_integral_dict = dict([(key, idx) for idx, key in enumerate(feature_vector_labels_integral)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/outbrain/preprocessed/'+'feature_vector_labels_integral.txt', 'w') as output:\n",
    "    output.writelines('\\n'.join(feature_vector_labels_integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value) and str(field_value) != '-1':\n",
    "        feature_name = get_ohe_feature_name(field_name, field_value)\n",
    "        if feature_name in feature_vector_labels_dict:\n",
    "            feature_idx = feature_vector_labels_dict[feature_name]\n",
    "        else:\n",
    "            #Unpopular category value\n",
    "            feature_idx = feature_vector_labels_dict[get_ohe_feature_name(field_name, LESS_SPECIAL_CAT_VALUE)]\n",
    "            \n",
    "        feature_vector[feature_idx] = float(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_values(field_name, field_values, feature_vector):\n",
    "    for field_value in field_values:\n",
    "        set_feature_vector_cat_value(field_name, field_value, feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "        event_country, event_country_state,\n",
    "        ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "        geo_location_event, \n",
    "        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "        traffic_source_pv, advertiser_id, publisher_id,\n",
    "        campaign_id, document_id_event,\n",
    "        doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "        doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "        doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "        doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "        doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "        doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_dict['event_weekend']] = float(weekend)                                                      \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "            publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "            timestamp_event, \n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        # -1 to traffic_source and platform_event\n",
    "        if traffic_source_pv != None:\n",
    "            feature_vector[feature_vector_labels_dict[TRAFFIC_SOURCE_FV]] = int(traffic_source_pv - 1)\n",
    "        if platform_event != None:\n",
    "            feature_vector[feature_vector_labels_dict[EVENT_PLATFORM_FV]] = int(platform_event - 1)\n",
    "        \n",
    "        # set_feature_vector_cat_value(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_FV, event_country, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_STATE_FV, event_country_state, feature_vector)         \n",
    "        set_feature_vector_cat_value(EVENT_GEO_LOCATION_FV, geo_location_event, feature_vector)\n",
    "        # set_feature_vector_cat_value(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids, feature_vector)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector] ERROR PROCESSING FEATURE VECTOR! Params: {}\"\n",
    "            .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                event_country, event_country_state,\n",
    "                ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                geo_location_event, \n",
    "                doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                traffic_source_pv, advertiser_id, publisher_id,\n",
    "                campaign_id, document_id_event,\n",
    "                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "            e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_dict), feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ad_feature_vector_udf = F.udf(\n",
    "    lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "            user_entities, event_country, event_country_state, ad_id, document_id, \n",
    "            source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "            geo_location_event, \n",
    "            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "            traffic_source_pv, advertiser_id, publisher_id,\n",
    "            campaign_id, document_id_event,\n",
    "            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "            doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "        get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                event_country, event_country_state, \n",
    "                ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                geo_location_event, \n",
    "                doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                traffic_source_pv, advertiser_id, publisher_id,\n",
    "                campaign_id, document_id_event,\n",
    "                category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value_integral(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value): #and str(field_value) != '-1':\n",
    "        feature_vector[feature_vector_labels_integral_dict[field_name]] = float(field_value)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_top_multi_values_integral(\n",
    "        field_name, values, confidences, feature_vector, top=5):\n",
    "    top_values = list(filter(lambda z: z != -1, \n",
    "            map(lambda y: y[0], sorted(zip(values, confidences), key=lambda x: -x[1]))))[:top]\n",
    "    for idx, field_value in list(enumerate(top_values)):\n",
    "        set_feature_vector_cat_value_integral(\n",
    "            '{}_{}'.format(field_name, idx+1), field_value, feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_feature_vector_integral(\n",
    "        user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "        event_country, event_country_state,\n",
    "        ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "        geo_location_event, \n",
    "        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "        traffic_source_pv, advertiser_id, publisher_id,\n",
    "        campaign_id, document_id_event,\n",
    "        doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "        doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "        doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "        doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "        doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "        doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "       \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value: \n",
    "            feature_vector[feature_vector_labels_integral_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value_integral(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_integral_dict['event_weekend']] = float(weekend)               \n",
    "                                        \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "            publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(\n",
    "            user_views_count, user_categories, user_topics, user_entities, \n",
    "            timestamp_event, \n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "            output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "        \n",
    "        #Process code for event_country\n",
    "        if event_country in event_country_values_counts:\n",
    "            event_country_code = event_country_values_counts[event_country]\n",
    "        else:\n",
    "            event_country_code = event_country_values_counts[LESS_SPECIAL_CAT_VALUE]                        \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_FV, event_country_code, feature_vector)\n",
    "        \n",
    "        #Process code for event_country_state\n",
    "        if event_country_state in event_country_state_values_counts:\n",
    "            event_country_state_code = event_country_state_values_counts[event_country_state]\n",
    "        else:\n",
    "            event_country_state_code = event_country_state_values_counts[LESS_SPECIAL_CAT_VALUE]         \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_STATE_FV, event_country_state_code, feature_vector)\n",
    "                \n",
    "        #Process code for geo_location_event\n",
    "        if geo_location_event in event_geo_location_values_counts:\n",
    "            geo_location_event_code = event_geo_location_values_counts[geo_location_event]\n",
    "        else:\n",
    "            geo_location_event_code = event_geo_location_values_counts[LESS_SPECIAL_CAT_VALUE]\n",
    "        \n",
    "        # -1 to traffic_source and platform_event\n",
    "        if traffic_source_pv != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict[TRAFFIC_SOURCE_FV]] = int(traffic_source_pv - 1)\n",
    "        if platform_event != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict[EVENT_PLATFORM_FV]] = int(platform_event - 1)\n",
    "        \n",
    "        set_feature_vector_cat_value_integral(EVENT_GEO_LOCATION_FV, geo_location_event_code, feature_vector)   \n",
    "         \n",
    "        # set_feature_vector_cat_value_integral(TRAFFIC_SOURCE_FV, traffic_source_pv - 1, feature_vector)        \n",
    "        # set_feature_vector_cat_value_integral(EVENT_PLATFORM_FV, platform_event - 1, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "                \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, doc_ad_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, doc_ad_top_confidence_levels, feature_vector, top=3)\n",
    "        \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, doc_event_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, doc_event_top_confidence_levels, feature_vector, top=3)                           \n",
    "        \n",
    "        #Process codes for doc_ad_entity_ids\n",
    "        doc_ad_entity_ids_codes = [doc_entity_id_values_counts[x] \n",
    "            if x in doc_entity_id_values_counts \n",
    "            else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "            for x in doc_ad_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids_codes, doc_ad_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        \n",
    "        #Process codes for doc_event_entity_ids\n",
    "        doc_event_entity_ids_codes = [doc_entity_id_values_counts[x] \n",
    "            if x in doc_entity_id_values_counts \n",
    "            else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "            for x in doc_event_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids_codes, doc_event_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "      \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector_integral] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "           .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "             event_country, event_country_state,\n",
    "             ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "             geo_location_event, \n",
    "             doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "             traffic_source_pv, advertiser_id, publisher_id,\n",
    "             campaign_id, document_id_event,\n",
    "             doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "             doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "             doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "             doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "             doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "             doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "         e)\n",
    "  \n",
    "    return SparseVector(len(feature_vector_labels_integral_dict), feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ad_feature_vector_integral_udf = F.udf(\n",
    "    lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "            user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "            doc_ad_publish_time, timestamp_event, platform_event,\n",
    "            geo_location_event, \n",
    "            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "            traffic_source_pv, advertiser_id, publisher_id,\n",
    "            campaign_id, document_id_event,\n",
    "            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "            doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "        get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                event_country, event_country_state, \n",
    "                ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                geo_location_event, \n",
    "                doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                traffic_source_pv, advertiser_id, publisher_id,\n",
    "                campaign_id, document_id_event,\n",
    "                category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "            VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Train set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_enriched_df = train_set_df \\\n",
    "  .join(documents_categories_grouped_df, \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), \n",
    "    how='left') \\\n",
    "  .join(documents_topics_grouped_df, \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), \n",
    "    how='left') \\\n",
    "  .join(documents_entities_grouped_df, \n",
    "    on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), \n",
    "    how='left') \\\n",
    "  .join(documents_categories_grouped_df \\\n",
    "      .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "      .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "      .alias('documents_event_categories_grouped'), \n",
    "    on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "    how='left') \\\n",
    "  .join(documents_topics_grouped_df \\\n",
    "      .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "      .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "      .alias('documents_event_topics_grouped'), \n",
    "    on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "    how='left') \\\n",
    "  .join(documents_entities_grouped_df \\\n",
    "      .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "      .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "      .alias('documents_event_entities_grouped'), \n",
    "    on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "    how='left') \\\n",
    "  .select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "      'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',\n",
    "      'publish_time', 'ad_id','document_id_promo','clicked',   \n",
    "      'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "      'campaign_id', 'document_id_event',\n",
    "      'traffic_source_pv',                                          \n",
    "      int_list_null_to_empty_list_udf('doc_event_category_id_list') \n",
    "        .alias('doc_event_category_id_list'),\n",
    "      float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list')\n",
    "        .alias('doc_event_confidence_level_cat_list'),\n",
    "      int_list_null_to_empty_list_udf('doc_event_topic_id_list')\n",
    "        .alias('doc_event_topic_id_list'),\n",
    "      float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list')\n",
    "        .alias('doc_event_confidence_level_top_list'),\n",
    "      str_list_null_to_empty_list_udf('doc_event_entity_id_list')\n",
    "        .alias('doc_event_entity_id_list'),\n",
    "      float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list')\n",
    "        .alias('doc_event_confidence_level_ent_list'),\n",
    "      int_null_to_minus_one_udf('source_id').alias('source_id'),\n",
    "      int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "      int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "      float_list_null_to_empty_list_udf('confidence_level_cat_list')\n",
    "        .alias('confidence_level_cat_list'), \n",
    "      int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "      float_list_null_to_empty_list_udf('confidence_level_top_list')\n",
    "        .alias('confidence_level_top_list'), \n",
    "      str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "      float_list_null_to_empty_list_udf('confidence_level_ent_list')\n",
    "        .alias('confidence_level_ent_list')) \\\n",
    "  .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "  .withColumnRenamed('categories', 'user_categories') \\\n",
    "  .withColumnRenamed('topics', 'user_topics') \\\n",
    "  .withColumnRenamed('entities', 'user_entities') \\\n",
    "  .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "  .withColumnRenamed('views', 'user_views_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(display_id=8264198, uuid_event='10015959b777b6', event_country='US', event_country_state='US', platform_event=2, source_id_doc_event=482, publisher_doc_event=65, publish_time_doc_event=datetime.datetime(2014, 8, 5, 16, 0), publish_time=datetime.datetime(2014, 1, 28, 16, 0), ad_id=123742, document_id_promo=1148731, clicked=0, geo_location_event='US', advertiser_id=571, publisher_id=523, campaign_id=15889, document_id_event=786004, traffic_source_pv=1, doc_event_category_id_list=[1403, 1402], doc_event_confidence_level_cat_list=[0.8077853918075562, 0.061461932957172394], doc_event_topic_id_list=[82], doc_event_confidence_level_top_list=[0.25338107347488403], doc_event_entity_id_list=[], doc_event_confidence_level_ent_list=[], source_id=478, timestamp_event=566415049, category_id_list=[1504, 1503], confidence_level_cat_list=[0.8669483661651611, 0.06596346199512482], topic_id_list=[285], confidence_level_top_list=[0.16625766456127167], entity_id_list=['a9dc59dee4759f637b1cfe64e551c9a3'], confidence_level_ent_list=[0.23439446091651917], uuid=None, user_doc_ids_viewed=None, user_views_count=None, user_categories=None, user_topics=None, user_entities=None, dummyUserProfiles=None),\n",
       " Row(display_id=8264198, uuid_event='10015959b777b6', event_country='US', event_country_state='US', platform_event=2, source_id_doc_event=482, publisher_doc_event=65, publish_time_doc_event=datetime.datetime(2014, 8, 5, 16, 0), publish_time=None, ad_id=114910, document_id_promo=1149641, clicked=0, geo_location_event='US', advertiser_id=1341, publisher_id=178, campaign_id=10609, document_id_event=786004, traffic_source_pv=1, doc_event_category_id_list=[1403, 1402], doc_event_confidence_level_cat_list=[0.8077853918075562, 0.061461932957172394], doc_event_topic_id_list=[82], doc_event_confidence_level_top_list=[0.25338107347488403], doc_event_entity_id_list=[], doc_event_confidence_level_ent_list=[], source_id=7918, timestamp_event=566415049, category_id_list=[1403, 1514], confidence_level_cat_list=[0.8557798862457275, 0.06511368602514267], topic_id_list=[250, 136, 20, 202, 27, 243, 84, 74], confidence_level_top_list=[0.04876498878002167, 0.04213833808898926, 0.03144427016377449, 0.025217467918992043, 0.023232582956552505, 0.01782245561480522, 0.013244603760540485, 0.00908795464783907], entity_id_list=['b058b9a1f679f45c5bca7d73c37f888a', 'ebea09bff8d7562cda14912ffcf74b44', '3e2329d0532b88665cdd7c9625786738'], confidence_level_ent_list=[0.8866181373596191, 0.744560182094574, 0.2706966698169708], uuid=None, user_doc_ids_viewed=None, user_views_count=None, user_categories=None, user_topics=None, user_entities=None, dummyUserProfiles=None)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_enriched_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_feature_vectors_df = train_set_enriched_df \\\n",
    "  .withColumn('feature_vector', \n",
    "    get_ad_feature_vector_integral_udf(\n",
    "      'user_doc_ids_viewed',\n",
    "      'user_views_count',\n",
    "      'user_categories', \n",
    "      'user_topics', \n",
    "      'user_entities', \n",
    "      'event_country', \n",
    "      'event_country_state',\n",
    "      'ad_id', \n",
    "      'document_id_promo', \n",
    "      'source_id', \n",
    "      'publish_time', \n",
    "      'timestamp_event', \n",
    "      'platform_event',\n",
    "      'geo_location_event', \n",
    "      'source_id_doc_event', \n",
    "      'publisher_doc_event',\n",
    "      'publish_time_doc_event',\n",
    "      'traffic_source_pv',\n",
    "      'advertiser_id', \n",
    "      'publisher_id',\n",
    "      'campaign_id',\n",
    "      'document_id_event',\n",
    "      'category_id_list', \n",
    "      'confidence_level_cat_list', \n",
    "      'topic_id_list', \n",
    "      'confidence_level_top_list',\n",
    "      'entity_id_list', \n",
    "      'confidence_level_ent_list',\n",
    "      'doc_event_category_id_list',\n",
    "      'doc_event_confidence_level_cat_list',\n",
    "      'doc_event_topic_id_list',\n",
    "      'doc_event_confidence_level_top_list',\n",
    "      'doc_event_entity_id_list',\n",
    "      'doc_event_confidence_level_ent_list')) \\\n",
    "  .select(F.col('uuid_event').alias('uuid'), 'display_id', 'ad_id', 'document_id_event',\n",
    "    F.col('document_id_promo').alias('document_id'), F.col('clicked').alias('label'),\n",
    "    'feature_vector') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(uuid='10015959b777b6', display_id=8264198, ad_id=130952, document_id_event=786004, document_id=1286844, label=1, feature_vector=SparseVector(103, {0: 0.0, 3: 91964.0, 4: 110688.0, 5: 685.0, 6: 5.0, 8: 0.2624, 9: 0.6881, 10: 0.1806, 11: 0.2484, 12: 0.567, 13: 0.1409, 14: 0.2363, 15: 0.5443, 16: 0.1286, 17: 0.2363, 18: 0.5443, 19: 0.1286, 20: 0.244, 21: 0.555, 22: 0.1354, 26: 0.2363, 27: 0.5443, 28: 0.1286, 29: 0.2363, 30: 0.5443, 31: 0.1286, 32: 0.2484, 33: 0.0514, 34: 0.0128, 35: 0.2484, 36: 0.0514, 37: 0.0128, 38: 0.1903, 39: 0.0046, 40: 0.0009, 41: 0.192, 42: 0.0042, 43: 0.0008, 44: 0.1693, 45: 0.146, 46: 0.0247, 47: 0.164, 48: 0.1311, 49: 0.0215, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0001, 64: 0.0, 68: 131.0, 69: 1505.0, 70: 1515.0, 72: 143.0, 73: 131.0, 74: 138.0, 81: 1346.0, 82: 1223.0, 83: 1403.0, 84: 1402.0, 86: 82.0, 95: 65.0, 96: 482.0, 97: 18595447.0, 98: 758487.0, 99: 758487.0, 100: 5.0, 101: 1.0, 102: 0.0})),\n",
       " Row(uuid='10015959b777b6', display_id=8264198, ad_id=64255, document_id_event=786004, document_id=747047, label=0, feature_vector=SparseVector(103, {0: 0.0, 3: 44105.0, 4: 44132.0, 5: 685.0, 6: 5.0, 7: 435.0, 8: 0.1865, 9: 0.6439, 10: 0.1201, 11: 0.1865, 12: 0.6022, 13: 0.1123, 14: 0.1808, 15: 0.4286, 16: 0.0775, 17: 0.1808, 18: 0.4286, 19: 0.0775, 20: 0.1865, 21: 0.5779, 22: 0.1078, 26: 0.1808, 27: 0.4286, 28: 0.0775, 29: 0.1808, 30: 0.4286, 31: 0.0775, 32: 0.1856, 33: 0.0001, 34: 0.0, 35: 0.1856, 36: 0.0001, 37: 0.0, 38: 0.1821, 39: 0.0079, 40: 0.0014, 41: 0.1882, 42: 0.0091, 43: 0.0017, 44: 0.2088, 45: 0.1339, 46: 0.028, 47: 0.2139, 48: 0.1326, 49: 0.0284, 59: 0.0, 60: 0.0004, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 68: 1077.0, 69: 1302.0, 70: 1100.0, 72: 1.0, 81: 1956.0, 82: 7823.0, 83: 1403.0, 84: 1402.0, 86: 82.0, 95: 65.0, 96: 482.0, 97: 18595447.0, 98: 758487.0, 99: 758487.0, 100: 5.0, 101: 1.0, 102: 0.0}))]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_feature_vectors_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral_eval'\n",
    "else:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs:/user/lzhao/data/outbrain/preprocessed/'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_BUCKET_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_feature_vectors_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-fe82491d56c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set_feature_vectors_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_BUCKET_FOLDER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain_feature_vector_gcs_folder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_feature_vectors_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leak(max_timestamp_pv_leak, timestamp_event):\n",
    "    return max_timestamp_pv_leak >= 0 and max_timestamp_pv_leak >= timestamp_event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_leak_udf = F.udf(lambda max_timestamp_pv_leak, timestamp_event: int(is_leak(max_timestamp_pv_leak, timestamp_event)), IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_set_enriched_df = data_df.select(\n",
    "    'display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "    'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',     \n",
    "    'publish_time',\n",
    "    'ad_id','document_id_promo','clicked',  \n",
    "    'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "    'campaign_id', 'document_id_event',\n",
    "    'traffic_source_pv',                                           \n",
    "    int_list_null_to_empty_list_udf('doc_event_category_id_list')\n",
    "      .alias('doc_event_category_id_list'),\n",
    "    float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list')\n",
    "      .alias('doc_event_confidence_level_cat_list'),\n",
    "    int_list_null_to_empty_list_udf('doc_event_topic_id_list')\n",
    "      .alias('doc_event_topic_id_list'),\n",
    "    float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list')\n",
    "      .alias('doc_event_confidence_level_top_list'),\n",
    "    str_list_null_to_empty_list_udf('doc_event_entity_id_list')\n",
    "      .alias('doc_event_entity_id_list'),\n",
    "    float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list')\n",
    "      .alias('doc_event_confidence_level_ent_list'),\n",
    "    int_null_to_minus_one_udf('source_id')\n",
    "      .alias('source_id'),                                   \n",
    "    int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "    int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "    float_list_null_to_empty_list_udf('confidence_level_cat_list')\n",
    "      .alias('confidence_level_cat_list'), \n",
    "    int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "    float_list_null_to_empty_list_udf('confidence_level_top_list')\n",
    "      .alias('confidence_level_top_list'), \n",
    "    str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "    float_list_null_to_empty_list_udf('confidence_level_ent_list')\n",
    "      .alias('confidence_level_ent_list'),\n",
    "    int_null_to_minus_one_udf('max_timestamp_pv').alias('max_timestamp_pv_leak')) \\\n",
    "  .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "  .withColumnRenamed('categories', 'user_categories') \\\n",
    "  .withColumnRenamed('topics', 'user_topics') \\\n",
    "  .withColumnRenamed('entities', 'user_entities') \\\n",
    "  .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "  .withColumnRenamed('views', 'user_views_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_set_feature_vectors_df = test_validation_set_enriched_df \\\n",
    "  .withColumn('feature_vector', \n",
    "    get_ad_feature_vector_integral_udf(\n",
    "      'user_doc_ids_viewed', \n",
    "      'user_views_count',\n",
    "      'user_categories', \n",
    "      'user_topics', \n",
    "      'user_entities', \n",
    "      'event_country', \n",
    "      'event_country_state',\n",
    "      'ad_id', \n",
    "      'document_id_promo', \n",
    "      'source_id', \n",
    "      'publish_time', \n",
    "      'timestamp_event', \n",
    "      'platform_event',\n",
    "      'geo_location_event', \n",
    "      'source_id_doc_event', \n",
    "      'publisher_doc_event',\n",
    "      'publish_time_doc_event',\n",
    "      'traffic_source_pv',\n",
    "      'advertiser_id', \n",
    "      'publisher_id',\n",
    "      'campaign_id',\n",
    "      'document_id_event',\n",
    "      'category_id_list', \n",
    "      'confidence_level_cat_list', \n",
    "      'topic_id_list', \n",
    "      'confidence_level_top_list',\n",
    "      'entity_id_list', \n",
    "      'confidence_level_ent_list',\n",
    "      'doc_event_category_id_list',\n",
    "      'doc_event_confidence_level_cat_list',\n",
    "      'doc_event_topic_id_list',\n",
    "      'doc_event_confidence_level_top_list',\n",
    "      'doc_event_entity_id_list',\n",
    "      'doc_event_confidence_level_ent_list')) \\\n",
    "  .select(F.col('uuid').alias('uuid'), 'display_id', 'ad_id', 'document_id_event',\n",
    "    F.col('document_id_promo').alias('document_id'), F.col('clicked').alias('label'),\n",
    "    is_leak_udf('max_timestamp_pv_leak','timestamp_event').alias('is_leak'),\n",
    "    'feature_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'validation_feature_vectors_integral'\n",
    "else:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'test_feature_vectors_integral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
